{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings\n",
    "We are retraining embeddings based on the cbow architecture in word2vec. \n",
    "\n",
    "First, tokenize the combined text8 and ms-marco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before filtering: 62266764\n",
      "Number of words after filtering: 61436223\n",
      "Number of words in vocab: 127665\n",
      "Number of words in vocab_to_int: 127666\n",
      "Number of words in int_to_vocab: 127666\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import preprocess, create_lookup_tables\n",
    "import pickle\n",
    "\n",
    "# open the combined_text8_msmarco.txt file and read the text\n",
    "with open('./data/combined_text8_msmarco.txt', 'r') as f:\n",
    "    combined_text = f.read()\n",
    "\n",
    "# tokenize the text and save as a .pkl file\n",
    "combined_tokens = preprocess(combined_text)\n",
    "with open('./data/temp/combined_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_tokens, f)\n",
    "\n",
    "# create the lookup tables\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(combined_tokens)\n",
    "\n",
    "# save the lookup tables to a .pkl file\n",
    "with open('./data/temp/combined_vocab_to_int.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_to_int, f)\n",
    "\n",
    "with open('./data/temp/combined_int_to_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(int_to_vocab, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our new vocabulary from the combined text8 wiki data and ms-marco data, let's generate the embeddings using the CBOW architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(corpus[:100])\n",
    "\n",
    "# Generate the training data from the corpus\n",
    "# The training data looks like a list of tuples,\n",
    "# where each tuple contains a list of context words and the target word (not the IDs)\n",
    "\n",
    "\n",
    "def generate_training_data(corpus):\n",
    "   data = []\n",
    "\n",
    "   # start from index 2 and end 2 positions before the last word\n",
    "   # this ensures we always have 2 words before and after the target word\n",
    "   # for a 5-len sliding window\n",
    "\n",
    "\n",
    "   for i in range(2, len(corpus) - 2):\n",
    "       # Get the context words\n",
    "       # 'i' is the index of the target word\n",
    "       # [i-2:i] gets the two words before the target word\n",
    "       # [i+1:i+3] gets the two words after the target word\n",
    "       context_words = corpus[i-2:i] + corpus[i+1:i+3]\n",
    "      \n",
    "       # Get the target word\n",
    "       target_word = corpus[i]\n",
    "\n",
    "\n",
    "       # Append the tuple to the data list\n",
    "       data.append((context_words, target_word))\n",
    "\n",
    "\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW training data generated\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "\n",
    "# load the corpus\n",
    "#with open('./data/temp/combined_corpus.pkl', 'rb') as f:\n",
    "    #corpus = pickle.load(f)\n",
    "\n",
    "training_data = generate_training_data(combined_tokens)\n",
    "\n",
    "print(\"CBOW training data generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training data to a .pkl file\n",
    "with open('./data/temp/combined_training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m training_data  \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "del training_data  # Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['esp', 'magnetic', 'or', 'electric'], 'poles'),\n",
       " (['magnetic', 'poles', 'electric', 'charge'], 'or'),\n",
       " (['poles', 'or', 'charge', '<PERIOD>'], 'electric'),\n",
       " (['or', 'electric', '<PERIOD>', '3'], 'charge'),\n",
       " (['electric', 'charge', '3', '<PERIOD>'], '<PERIOD>'),\n",
       " (['charge', '<PERIOD>', '<PERIOD>', '<LEFT_PAREN>'], '3'),\n",
       " (['<PERIOD>', '3', '<LEFT_PAREN>', 'general'], '<PERIOD>'),\n",
       " (['3', '<PERIOD>', 'general', 'physics'], '<LEFT_PAREN>'),\n",
       " (['<PERIOD>', '<LEFT_PAREN>', 'physics', '<RIGHT_PAREN>'], 'general'),\n",
       " (['<LEFT_PAREN>', 'general', '<RIGHT_PAREN>', 'the'], 'physics'),\n",
       " (['general', 'physics', 'the', 'particular'], '<RIGHT_PAREN>'),\n",
       " (['physics', '<RIGHT_PAREN>', 'particular', 'state'], 'the'),\n",
       " (['<RIGHT_PAREN>', 'the', 'state', 'of'], 'particular'),\n",
       " (['the', 'particular', 'of', 'a'], 'state'),\n",
       " (['particular', 'state', 'a', 'part'], 'of'),\n",
       " (['state', 'of', 'part', 'of'], 'a'),\n",
       " (['of', 'a', 'of', 'a'], 'part'),\n",
       " (['a', 'part', 'a', 'body'], 'of'),\n",
       " (['part', 'of', 'body', 'or'], 'a'),\n",
       " (['of', 'a', 'or', 'system'], 'body'),\n",
       " (['a', 'body', 'system', 'that'], 'or'),\n",
       " (['body', 'or', 'that', 'has'], 'system'),\n",
       " (['or', 'system', 'has', 'polarity'], 'that'),\n",
       " (['system', 'that', 'polarity', '<COLON>'], 'has'),\n",
       " (['that', 'has', '<COLON>', 'an'], 'polarity'),\n",
       " (['has', 'polarity', 'an', 'electrode'], '<COLON>'),\n",
       " (['polarity', '<COLON>', 'electrode', 'with'], 'an'),\n",
       " (['<COLON>', 'an', 'with', 'positive'], 'electrode'),\n",
       " (['an', 'electrode', 'positive', 'polarity'], 'with'),\n",
       " (['electrode', 'with', 'polarity', '<PERIOD>'], 'positive')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick eyeball check to see if the training data is correct\n",
    "# show the last 30 tuples in the training data\n",
    "training_data[-30:]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"\n",
    "UNCOMMENT THIS SECTION TO PUSH THE DATA TO HUGGING FACE\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Push the combined corpus, int_to_vocab, and vocab_to_int to Hugging Face\n",
    "# Make sure have a HF account and token, save token to a .env file\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "pt_files = glob.glob(\"./data/temp/*\")\n",
    "for file in pt_files:\n",
    "    print(\"Currently uploading\", file)\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=os.path.basename(file),\n",
    "        repo_id=\"titaneve/text8-and-msmarco\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    print(f\"Successfully uploaded {file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"\n",
    " UNCOMMENT THIS SECTION IF LOADING FROM LOCAL \n",
    "\"\"\"\n",
    "\n",
    "# Load the word_to_id and training_data from the temp folder\n",
    "with open('./data/temp/combined_vocab_to_int.pkl', 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "\n",
    "with open('./data/temp/combined_training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "word_to_id[:10]\n",
    "training_data[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127666\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

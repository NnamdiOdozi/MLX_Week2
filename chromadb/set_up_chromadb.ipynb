{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we set up chromadb, a vector database used to store document embeddings for easy retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Make sure this notebook is running on the GPU\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Stream the dataset from Hugging Face\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\")\n",
    "\n",
    "# Extrac the train, test, and validation datasets into a list of dictionaries\n",
    "train_ds = ds[\"train\"]['passages']\n",
    "test_ds = ds[\"test\"]['passages']\n",
    "validation_ds = ds[\"validation\"]['passages']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the list of dictionaries, you want the passage_text passages and corresponding url\n",
    "passages = []\n",
    "urls = []\n",
    "for item in test_ds:\n",
    "    for passage in item['passage_text']:\n",
    "        passages.append(passage)\n",
    "    \n",
    "    for url in item['url']:\n",
    "        urls.append(url)\n",
    "    \n",
    "# create a list of ids the length of the passages list\n",
    "ids = [str(i) for i in range(len(passages))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_collection(ds):\n",
    "    passages = []\n",
    "    urls = []\n",
    "\n",
    "    for item in ds:\n",
    "        for passage in item['passage_text']:\n",
    "            passages.append(passage)\n",
    "        for url in item['url']:\n",
    "            urls.append(url)\n",
    "\n",
    "    return passages, urls\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before joining:\n",
      "train 676193\n",
      "test 79176\n",
      "validation 82360\n",
      "urls train 676193\n",
      "urls test 79176\n",
      "urls validation 82360\n",
      "after joining:\n",
      "all passages 837729\n",
      "all urls 837729\n",
      "ids 676193\n",
      "metadatas 676193\n"
     ]
    }
   ],
   "source": [
    "passages_train, urls_train = create_data_for_collection(train_ds)\n",
    "passages_test, urls_test = create_data_for_collection(test_ds)\n",
    "passages_validation, urls_validation = create_data_for_collection(validation_ds)\n",
    "\n",
    "print(\"before joining:\")\n",
    "print(\"train\", len(passages_train))\n",
    "print(\"test\", len(passages_test))\n",
    "print(\"validation\", len(passages_validation))\n",
    "print(\"urls train\", len(urls_train))\n",
    "print(\"urls test\", len(urls_test))\n",
    "print(\"urls validation\", len(urls_validation))\n",
    "\n",
    "# join the passages\n",
    "all_passages = passages_train + passages_test + passages_validation\n",
    "all_urls = urls_train + urls_test + urls_validation\n",
    "\n",
    "print(\"after joining:\")\n",
    "print(\"all passages\", len(all_passages))\n",
    "print(\"all urls\", len(all_urls))\n",
    "\n",
    "\n",
    "# Conver the list of urls to a list of dictionaries for ChromaDB\n",
    "metadatas = [{\"url\": url} for url in urls_train]\n",
    "ids = [str(i) for i in range(len(passages_train))]\n",
    "print(\"ids\", len(ids))\n",
    "print(\"metadatas\", len(metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create dummy data assuming we will have a document embedding pair incoming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents 2000\n",
      "document_embeddings 2000\n",
      "dummy_ids 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Create a list of dummy document embeddings with 128 dimensions\n",
    "document_embeddings = [np.random.rand(128) for _ in range(2000)]\n",
    "\n",
    "# Create a list of dummy documents randomly extracted from passages_train\n",
    "documents = [passages_train[i] for i in random.sample(range(len(passages_train)), 2000)]\n",
    "\n",
    "# Create corresponding ids\n",
    "dummy_ids = [str(i) for i in range(len(documents))]\n",
    "\n",
    "print(\"documents\", len(documents))\n",
    "print(\"document_embeddings\", len(document_embeddings))\n",
    "print(\"dummy_ids\", len(dummy_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have all the passages, urls, and ids, we create a document collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "# First create a persistent client to store the data\n",
    "persistent_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "persistent_client.delete_collection(\"test_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 20/20 [00:01<00:00, 16.98it/s, processed=2000/2000 passages]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 20/20 [00:19<00:00, 16.98it/s, processed=2000/2000 passages]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the collection and add your data\n",
    "collection = persistent_client.create_collection(\n",
    "    name=\"test_collection\", \n",
    "    metadata={\"hnsw:space\": \"cosine\", \"dimension\": 128}  # Match your embedding dimension\n",
    ")\n",
    "\n",
    "# Process in batches of 2000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Create progress bar for the total number of batches\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "progress_bar = tqdm(range(total_batches), desc=\"Processing batches\")\n",
    "\n",
    "for i in range(0, len(documents), BATCH_SIZE):\n",
    "    batch_end = min(i + BATCH_SIZE, len(documents))\n",
    "    \n",
    "    # Get batch of data\n",
    "    batch_passages = documents[i:batch_end]\n",
    "    #batch_metadatas = metadatas[i:batch_end]\n",
    "    batch_ids = dummy_ids[i:batch_end]\n",
    "    batch_embeddings = document_embeddings[i:batch_end]\n",
    "    \n",
    "    # Add batch to collection\n",
    "    collection.add(\n",
    "        documents=batch_passages,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings\n",
    "    )\n",
    "    \n",
    "    # Update progress bar\n",
    "    progress_bar.update()\n",
    "    progress_bar.set_postfix({\"processed\": f\"{batch_end}/{len(documents)} passages\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Start the server\n",
    "client = chromadb.HttpClient(host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "#client.delete_collection(\"ms_marcov1.1_collection\")\n",
    "# create a new collection\n",
    "collection = client.create_collection(\"ms_marcov1.1_collection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 128, got 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is the president of the United States?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:218\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m query_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_query_request(\n\u001b[1;32m    208\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    209\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    216\u001b[0m )\n\u001b[0;32m--> 218\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhere\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhere_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_query_response(\n\u001b[1;32m    230\u001b[0m     response\u001b[38;5;241m=\u001b[39mquery_results, include\u001b[38;5;241m=\u001b[39mquery_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m )\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/chromadb/api/rust.py:497\u001b[0m, in \u001b[0;36mRustBindingsAPI._query\u001b[0;34m(self, collection_id, query_embeddings, n_results, where, where_document, include, tenant, database)\u001b[0m\n\u001b[1;32m    482\u001b[0m query_amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(query_embeddings)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduct_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[1;32m    484\u001b[0m     CollectionQueryEvent(\n\u001b[1;32m    485\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m )\n\u001b[0;32m--> 497\u001b[0m rust_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[1;32m    509\u001b[0m     ids\u001b[38;5;241m=\u001b[39mrust_response\u001b[38;5;241m.\u001b[39mids,\n\u001b[1;32m    510\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mrust_response\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     distances\u001b[38;5;241m=\u001b[39mrust_response\u001b[38;5;241m.\u001b[39mdistances,\n\u001b[1;32m    517\u001b[0m )\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Collection expecting embedding with dimension of 128, got 384"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\n",
    "        \"Who is the president of the United States?\"\n",
    "    ],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we learn how to store embeddings in a local storage and link it to the Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

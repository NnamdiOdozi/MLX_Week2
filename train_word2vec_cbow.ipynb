{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import tqdm\n",
    "import wandb\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyCBOWDataset(Dataset):\n",
    "    def __init__(self, filepath, word_to_id):\n",
    "        self.word_to_id = word_to_id\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = json.loads(self.lines[idx])\n",
    "        context_ids = [self.word_to_id[w] for w in context]\n",
    "        target_id = self.word_to_id[target]\n",
    "        return torch.tensor(context_ids), torch.tensor(target_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "   def __init__(self, data, word_to_id):\n",
    "       self.data = data\n",
    "       self.word_to_id = word_to_id\n",
    "\n",
    "\n",
    "   # overriding the __len__ method to tell PyTorch how many samples you have\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "\n",
    "\n",
    "   # overriding the __getitem__ method\n",
    "   # to tell PyTorch how to retrieve a specific sample and convert it to the format your model expects\n",
    "   def __getitem__(self, idx):\n",
    "       context, target = self.data[idx]\n",
    "       context_ids = torch.tensor([self.word_to_id[word] for word in context], dtype=torch.long)\n",
    "       target_id = torch.tensor(self.word_to_id[target], dtype=torch.long)\n",
    "       return context_ids, target_id\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "   def __init__(self, vocab_size, embedding_dim):\n",
    "       super(CBOW, self).__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, inputs):\n",
    "       embed = self.embedding(inputs)\n",
    "       embed = embed.mean(dim=1)\n",
    "       out = self.linear(embed)\n",
    "       probs = F.log_softmax(out, dim=1)\n",
    "       return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4000\n",
      "GPU Memory: 16.88 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:473: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevelyntants\u001b[0m (\u001b[33mevelyntants-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250423_171350-st09eo6r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evelyntants-personal/train_word2vec_cbow/runs/st09eo6r' target=\"_blank\">2025_04_23__17_13_47</a></strong> to <a href='https://wandb.ai/evelyntants-personal/train_word2vec_cbow' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evelyntants-personal/train_word2vec_cbow' target=\"_blank\">https://wandb.ai/evelyntants-personal/train_word2vec_cbow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evelyntants-personal/train_word2vec_cbow/runs/st09eo6r' target=\"_blank\">https://wandb.ai/evelyntants-personal/train_word2vec_cbow/runs/st09eo6r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize settings\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "\n",
    "# load training data and word_to_id\n",
    "word_to_id = torch.load(\"./data/word_to_id.pt\")\n",
    "full_training_data = LazyCBOWDataset(\"./data/training_data.json\", word_to_id)\n",
    "\n",
    "train_size = int(0.7 * len(full_training_data))\n",
    "test_size = len(full_training_data) - train_size\n",
    "train_dataset, test_dataset = random_split(full_training_data, [train_size, test_size])\n",
    "\n",
    "# Initialize wandb with your configuration\n",
    "wandb.init(\n",
    "   project=\"train_word2vec_cbow\",\n",
    "   name=f\"{timestamp}\",\n",
    "   config={\n",
    "       # Model parameters\n",
    "       \"embedding_dim\": 100,\n",
    "       \"vocab_size\": 60000,\n",
    "      \n",
    "       # Training parameters\n",
    "       \"batch_size\": 128,\n",
    "       \"learning_rate\": 0.003,\n",
    "       \"num_epochs\": 5,\n",
    "       \"train_split\": 0.7,\n",
    "      \n",
    "       # Optimizer parameters\n",
    "       \"weight_decay\": 1e-5,\n",
    "      \n",
    "       # DataLoader parameters\n",
    "       \"num_workers\": 4,\n",
    "      \n",
    "       # Architecture details\n",
    "       \"model_type\": \"CBOW\",\n",
    "       \"context_size\": 4  # 2 words before + 2 words after\n",
    "   }\n",
    ")\n",
    "\n",
    "# Then use the config values throughout your code\n",
    "EMBEDDING_DIM = wandb.config.embedding_dim\n",
    "BATCH_SIZE = wandb.config.batch_size\n",
    "LEARNING_RATE = wandb.config.learning_rate\n",
    "NUM_EPOCHS = wandb.config.num_epochs\n",
    "TRAIN_SPLIT = wandb.config.train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create data loaders with GPU pinning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m   \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m   \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m   \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable pinning for faster GPU transfer\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Use multiple workers for data loading\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     12\u001b[0m    test_dataset,\n\u001b[1;32m     13\u001b[0m    batch_size\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m    num_workers\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:383\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 383\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=wandb.config.num_workers     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=wandb.config.num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(\n",
    "   vocab_size=wandb.config.vocab_size,\n",
    "   embedding_dim=wandb.config.embedding_dim)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=wandb.config.learning_rate,\n",
    "                            weight_decay=wandb.config.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Add evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "   model.eval()\n",
    "   total_loss = 0\n",
    "   with torch.no_grad():\n",
    "      for context, target in test_loader:\n",
    "           context, target = context.to(device), target.to(device)\n",
    "           output = model(context)\n",
    "           loss = criterion(output, target)\n",
    "           total_loss += loss.item()\n",
    "   return total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified training loop\n",
    "# Training Loop\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)\n",
    "    for inputs, targets in progress:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        wandb.log({'batch_loss': loss.item()})\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "    # Log epoch metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': avg_test_loss,\n",
    "    })\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{wandb.config.num_epochs}: Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # Save model checkpoint after every epoch\n",
    "    checkpoint_path = f\"../model/cbow_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    model_artifact = wandb.Artifact('model-weights', type='model')\n",
    "    model_artifact.add_file(checkpoint_path)\n",
    "    wandb.log_artifact(model_artifact)\n",
    "\n",
    "    # Save embeddings separately\n",
    "    embedding_weights = model.embedding.weight.data.cpu()\n",
    "    embedding_path = f\"../model/embeddings_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(embedding_weights, embedding_path)\n",
    "    embedding_artifact = wandb.Artifact('embeddings', type='embeddings')\n",
    "    embedding_artifact.add_file(embedding_path)\n",
    "    wandb.log_artifact(embedding_artifact)\n",
    "\n",
    "# Finish Wandb\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: PREPARE THE DATASET\n",
    "\n",
    "* get the dataset from MS Marco \n",
    "* extract queries and documents \n",
    "* generate triplets of queries, relevant(positive) documents, and irrelevant (negative) documents\n",
    "* tokenise your generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df_hn = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\")\n",
    "df_sn = load_dataset(\"cocoritzy/week_2_triplet_dataset_soft_negatives\")\n",
    "# dataset = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\", split=\"train[:10%]\") # 10% of the datab\n",
    "df_hn = df_hn[\"train\"].to_pandas()\n",
    "df_sn = df_sn[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length of all queries and passages because you neeed embedding dimensions to be the same\n",
    "\n",
    "def get_max_length(df, column_name):\n",
    "    lengths = []\n",
    "    for text in df[column_name]:\n",
    "        lengths.append(len(text))\n",
    "\n",
    "    print(f\"max length in {column_name}:\", max(lengths))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length in hn dataset\n",
    "hn_negatives_length = get_max_length(df_hn, \"negative_passage\")\n",
    "hn_positives_length = get_max_length(df_hn, \"positive_passage\")\n",
    "hn_query_length = get_max_length(df_hn, \"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length in sn dataset\n",
    "sn_negatives_length = get_max_length(df_sn, \"negative_passage\")\n",
    "sn_positives_length = get_max_length(df_sn, \"positive_passage\")\n",
    "sn_query_length = get_max_length(df_sn, \"query\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset you'll be using for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct triplet datasets\n",
    "triplet_hn = df_hn[[\"query\", \"positive_passage\", \"negative_passage\"]]\n",
    "triplet_sn = df_sn[[\"query\", \"positive_passage\", \"negative_passage\"]]\n",
    "triplet_hn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all columns into a single string\n",
    "marco_text = ' '.join(triplet_hn[\"query\"] + \" \" + triplet_hn[\"positive_passage\"] + \" \" + triplet_hn[\"negative_passage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data \n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    \n",
    "    # remove punctuation, number, and non-alphabetic characters\n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
    "    remove_numbers = re.sub(r'\\d+', '', remove_punctuation)\n",
    "    \n",
    "\n",
    "    lower_case_words = remove_numbers.lower()\n",
    "    # Split by whitespace and filter out empty strings\n",
    "    words = [word for word in lower_case_words.split() if word]\n",
    "\n",
    "    top_k = 30000\n",
    "    word_counts = Counter(words)\n",
    "    vocab = dict(word_counts.most_common(top_k))\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab.keys())}\n",
    "    id_to_word = {i: word for i, word in enumerate(vocab.keys())}\n",
    "\n",
    "    # Debugging: Print the unique vocabulary\n",
    "    \n",
    "    print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "    return vocab, word_to_id, id_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word_to_id, id_to_word = tokenize_text(marco_text)\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load in a pretrained word2vec\n",
    "* Coline's word2vec model embeddings to embed the query and document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure the GPU is being used \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from model import CBOW\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Print that the GPU is being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with text8 only\n",
    "model_path = hf_hub_download(repo_id=\"cocoritzy/cbow-upvotes_model\", filename=\"cbow_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve checkpoint \n",
    "checkpoint = torch.load(model_path, map_location=device) #A checkpoint is a file that saves the state of your model\n",
    "token_to_index = checkpoint[\"token_to_index\"]\n",
    "embedding_dim= checkpoint[\"embedding_dim\"]\n",
    "vocab_size = len(token_to_index)  # fill in actual size\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = CBOW(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "# Load the model parameters from the checkpoint\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval() # contains the trained weights of the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = model.embeddings.weight.data.clone() # Get the actual matrix of the pretrained vectors (the weights)\n",
    "\n",
    "# Quick sanity check\n",
    "print(type(model.embeddings))  # nn.Embedding -> shows you the whole embedding layer\n",
    "print(model.embeddings.weight.shape)  # torch.Size([vocab_size, embedding_dim])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: CREATE NEW EMBEDDINGS FOR THIS DATASET\n",
    "Now we should align the pretrained embeddings with my new word_to_id. We will do this by: \n",
    "* create an empty embedding\n",
    "* then filling the vectors from CBOW\n",
    "* if the word is not found in CBOW it gives a random vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "new_vocab_size = len(word_to_id)  # Your 30k vocab size\n",
    "embedding_dim = pretrained_weights.shape[1]\n",
    "\n",
    "# Fill with zeros\n",
    "embedding_matrix = torch.zeros((new_vocab_size, embedding_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, new_idx in word_to_id.items():\n",
    "    if word in token_to_index:\n",
    "        old_idx = token_to_index[word]\n",
    "        embedding_matrix[new_idx] = pretrained_weights[old_idx]\n",
    "    else:\n",
    "        # Word not found in CBOW — random vector\n",
    "        embedding_matrix[new_idx] = torch.randn(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity checker ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the embedding matrix\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[0])\n",
    "\n",
    "# Look at a random word in the embedding matrix\n",
    "print(id_to_word[100])\n",
    "print(embedding_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic similarity check\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "\n",
    "word1, word2 = 'takeoff', 'airplane'  # Replace with actual words\n",
    "if word1 in word_to_id and word2 in word_to_id:\n",
    "    idx1, idx2 = word_to_id[word1], word_to_id[word2]\n",
    "    similarity = cosine_similarity(embedding_matrix[idx1], embedding_matrix[idx2])\n",
    "    print(f\"Cosine similarity between {word1} and {word2}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks sensible enough to proceed with creating the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(30000, 100)\n",
    "embedding_layer.weight = nn.Parameter(embedding_matrix)\n",
    "embedding_layer.weight.requires_grad = False\n",
    "\n",
    "# Test the embedding layer\n",
    "test_word = 'takeoff'\n",
    "test_idx = word_to_id[test_word]\n",
    "print(embedding_layer(torch.tensor([test_idx])).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: USE EMBEDDINGS IN THE TWO TOWERS AND SET UP ALL THE CLASSES AND FUNCTIONS\n",
    "* remember you have two datasets: triplet_hn (hard negatives) and triple_sn (soft negatives)\n",
    "* Use the embedding layer to embed the queries and documents\n",
    "* for this, you are using __average pooling__ (not a RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model architecture. Both towers should return a pooled embedding for the query and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the model architecture aka the Two Towers with average pooling\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "\n",
    "    def forward(self, query_tokens):\n",
    "        # Embed the query tokens\n",
    "        embedded = self.embedding(query_tokens)\n",
    "        # Average the embeddings (for average pooling)\n",
    "        avg_query_embedding = torch.mean(embedded, dim=1)\n",
    "\n",
    "        return avg_query_embedding\n",
    "    \n",
    "class DocumentTower(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        \n",
    "    def forward(self, doc_tokens):\n",
    "        # Embed the document tokens\n",
    "        embedded = self.embedding(doc_tokens)\n",
    "        # Average the embeddings (for average pooling)\n",
    "        avg_doc_embedding = torch.mean(embedded, dim=1)\n",
    "\n",
    "        return avg_doc_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a distance function based on cosine similarity for the query and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distance function to compare the query and document embeddings\n",
    "# This function computes the similarity between the query and document embeddings, and return the top 5 most similar documents\n",
    "def compute_similarity(query_embedding, doc_embedding, k=5):\n",
    "\n",
    "    # Compute the cosine similarity between the query and document embeddings\n",
    "    similarity = F.cosine_similarity(query_embedding, doc_embedding)\n",
    "    \n",
    "    # Get the top k most similar documents\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, k)\n",
    "    return top_k_values, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class to make sure the dataset is loaded in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a dataset class for the triplet\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, all_queries, all_positive_passages, all_negative_passages, word_to_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries (list of str): List of query texts.\n",
    "            positive_passages (list of str): List of positive passage texts.\n",
    "            negative_passages (list of str): List of negative passage texts.\n",
    "            word_to_id (dict): Dictionary mapping words to their indices.\n",
    "        \"\"\"\n",
    "        self.queries = all_queries\n",
    "        self.positive_passages = all_positive_passages\n",
    "        self.negative_passages = all_negative_passages\n",
    "        self.word_to_id = word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the total number of samples\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        # Remove punctuation, numbers, and convert to lowercase\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "        text = text.lower()                  # Convert to lowercase\n",
    "        tokens = [word for word in text.split() if word]  # Split and remove empty tokens\n",
    "        return tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the query, positive passage, and negative passage for the given index\n",
    "        query = self.queries[idx]\n",
    "        positive_passage = self.positive_passages[idx]\n",
    "        negative_passage = self.negative_passages[idx]\n",
    "\n",
    "        # Tokenize the query and passages\n",
    "        query_tokens = self.tokenize_text(query)\n",
    "        positive_tokens = self.tokenize_text(positive_passage)\n",
    "        negative_tokens = self.tokenize_text(negative_passage)\n",
    "\n",
    "        # Conver to word ids\n",
    "        query_indices = torch.tensor([self.word_to_id[word] for word in query_tokens if word in self.word_to_id])\n",
    "        positive_indices = torch.tensor([self.word_to_id[word] for word in positive_tokens if word in self.word_to_id])\n",
    "        negative_indices = torch.tensor([self.word_to_id[word] for word in negative_tokens if word in self.word_to_id])\n",
    "\n",
    "        return query_indices, positive_indices, negative_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_length(tensor, length, padding_value=0):\n",
    "    # Calculate the padding size\n",
    "    pad_size = length - tensor.size(0)\n",
    "    if pad_size > 0:\n",
    "        # Pad the tensor to the specified length\n",
    "        return F.pad(tensor, (0, pad_size), value=padding_value)\n",
    "    else:\n",
    "        # If the tensor is already the desired length or longer, truncate it\n",
    "        return tensor[:length]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the batch into queries, positive passages, and negative passages\n",
    "    queries, positives, negatives = zip(*batch)\n",
    "    \n",
    "    # Define the fixed lengths (found from earlier in the notebook)\n",
    "    max_query_length = 144\n",
    "    max_positive_length = 1167\n",
    "    max_negative_length = 1039\n",
    "    \n",
    "    # Pad sequences to the fixed lengths\n",
    "    queries_padded = torch.stack([pad_to_length(q, max_query_length) for q in queries])\n",
    "    positives_padded = torch.stack([pad_to_length(p, max_positive_length) for p in positives])\n",
    "    negatives_padded = torch.stack([pad_to_length(n, max_negative_length) for n in negatives])\n",
    "    \n",
    "    return queries_padded, positives_padded, negatives_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Triplet Loss Function \n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.loss_fn = nn.TripletMarginLoss(margin=self.margin)\n",
    "\n",
    "    def forward(self, query_embedding, positive_embedding, negative_embedding):\n",
    "        return self.loss_fn(query_embedding, positive_embedding, negative_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHEEKY TESTING ZONE ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cheeky test before we move on \n",
    "\n",
    "triplet_dataset = TripletDataset(triplet_hn[\"query\"], triplet_hn[\"positive_passage\"], triplet_hn[\"negative_passage\"], word_to_id)\n",
    "\n",
    "# Create DataLoader with the collate function\n",
    "dataset_loader = DataLoader(triplet_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "query_tower = QueryTower(30000, 100)\n",
    "doc_tower = DocumentTower(30000, 100)\n",
    "\n",
    "# Run the model\n",
    "for query_indices, positive_indices, negative_indices in dataset_loader:\n",
    "    query_embedding = query_tower(query_indices)\n",
    "    positive_embedding = doc_tower(positive_indices)\n",
    "    negative_embedding = doc_tower(negative_indices)\n",
    "\n",
    "    print(\"Query Embedding:\", query_embedding)\n",
    "    print(\"Positive Embedding:\", positive_embedding)\n",
    "    print(\"Negative Embedding:\", negative_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial cheeky test showed that my tensors within queries and documents are different sizes. This is a problem because the torch.stack operation expects tensors in the batch to have the same size. So we'll fix it by padding the tensors based on the max lengths of the query and documents found in the earlier cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: SETUP MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Initialize settings\n",
    "torch.manual_seed(42)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y_%m_%d__%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"two_tower_model\", \n",
    "           entity=\"evelyntants\",\n",
    "           name=f\"{timestamp}\",\n",
    "           config={\n",
    "               # Model parameters\n",
    "               \"embedding_dim\": 100,\n",
    "               \"vocab_size\": 30000,\n",
    "                \n",
    "                # Training parameters\n",
    "                \"batch_size\": 128,\n",
    "                \"learning_rate\": 0.003,\n",
    "                \"num_epochs\": 5,\n",
    "                \"train_split\": 0.7,\n",
    "                \n",
    "                # Optimizer parameters\n",
    "                \"weight_decay\": 1e-5,\n",
    "                \n",
    "                # DataLoader parameters\n",
    "                \"num_workers\": 4,\n",
    "\n",
    "                # Triplet Loss margin\n",
    "                \"triplet_margin\": 1.0\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0ec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnamdi-odozi\u001b[0m (\u001b[33mnnamdi-odozi-ave-actuaries\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from load_models_and_data import load_vocabulary, load_embeddings, text_to_embeddings, calc_cosine_sim, calculate_embeddings, create_packed_batch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#from TwoTowerNN import QryTower, DocTower, TripletEmbeddingDataset, run_hyperparameter_tuning\n",
    "from TwinTowerGRU import QryTower, DocTower, TripletEmbeddingDataset, run_hyperparameter_tuning\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader,  SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b860fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba466e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading datasets from Hugging Face\n",
    "ds_soft_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_soft_negatives\")\n",
    "ds_hard_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1948ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and vocabulary...\n",
      "Loaded embeddings with shape: torch.Size([400000, 100])\n",
      "Loaded vocabulary with 399998 tokens\n",
      "Embedded text shape: torch.Size([26, 100])\n",
      "Embedding array with custom formatting:\n",
      "[[ 0.2616  0.4472 -0.0968 ... -0.4503  0.4952 -0.203 ]\n",
      " [ 0.1372 -0.5429  0.1942 ... -0.5206  0.2543 -0.2376]\n",
      " [-0.3046 -0.2365  0.1758 ... -0.8456 -0.0354  0.1704]\n",
      " ...\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]]\n",
      "Length is: 5\n"
     ]
    }
   ],
   "source": [
    "# Paths to your files\n",
    "embeddings_path = \"./downloaded_model/glove_embeddings.pt\" #set this to either own-trained cbow ones or to glove pre-trained ones\n",
    "vocab_path = \"./downloaded_model/glove_ids_to_words.csv\"\n",
    "\n",
    "# Load embeddings and vocabulary\n",
    "print(\"Loading embeddings and vocabulary...\")\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "word_to_idx = load_vocabulary(vocab_path)\n",
    "\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"Loaded vocabulary with {len(word_to_idx)} tokens\")\n",
    "\n",
    "# Example usage (uncomment when ready to test)\n",
    "sample_text = \"This is a test sentence\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "# Testing - Set numpy print options\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff426027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded text shape: torch.Size([26, 100])\n",
      "Embedding array with custom formatting:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Length is: 0\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a28244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg  = pd.DataFrame(ds_soft_neg['train'])\n",
    "df_hard_neg  = pd.DataFrame(ds_hard_neg['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b81dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 100])\n",
      "torch.Size([201, 100])\n"
     ]
    }
   ],
   "source": [
    "embedded_query, length = text_to_embeddings(df_soft_neg['query'][0], word_to_idx, embeddings, is_query=True)\n",
    "embedded_positive, length = text_to_embeddings(df_soft_neg['positive_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "embedded_negative, length = text_to_embeddings(df_soft_neg['negative_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "\n",
    "print(embedded_positive.shape)\n",
    "print(embedded_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ece943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = embedded_query.mean(dim=0)\n",
    "b = embedded_positive.mean(dim=0)\n",
    "c = embedded_negative.mean(dim=0)\n",
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cosine_similarity = F.cosine_similarity(a, c, dim=0)\n",
    "print(f\"Cosine similarity between query and positive passage: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Process the dataframe using apply just for first five rows\n",
    "# print(\"Calculating similarities... This may take a while depending on dataframe size.\")\n",
    "# similarities = df_soft_neg[0:5].progress_apply(\n",
    "#     lambda row: calculate_similarities(row, word_to_idx, embeddings), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Join the similarities to the dataframe\n",
    "# df_soft_neg_ext = pd.concat([df_soft_neg[0:5], similarities], axis=1)\n",
    "\n",
    "# # Show a sample of the results\n",
    "# #print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "#print(df_soft_neg_ext.head())\n",
    "#print(df_soft_neg_ext.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a02f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings... This may take a while depending on dataframe size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79704/79704 [02:32<00:00, 522.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id                                              query  \\\n",
      "0     19699                                        what is rba   \n",
      "1     19700                       was ronald reagan a democrat   \n",
      "2     19701  how long do you need for sydney and surroundin...   \n",
      "3     19702                    price to install tile in shower   \n",
      "4     19703                    why conversion observed in body   \n",
      "\n",
      "                                    positive_passage  \\\n",
      "0  Results-Based Accountability® (also known as R...   \n",
      "1  From Wikipedia, the free encyclopedia. A Reaga...   \n",
      "2  Sydney is the capital city of the Australian s...   \n",
      "3  1 Install ceramic tile floor to match shower-A...   \n",
      "4  Conversion disorder is a type of somatoform di...   \n",
      "\n",
      "                                    negative_passage  negative_from_query_id  \\\n",
      "0  I finally found some real salary data for phys...                   86595   \n",
      "1  The Pacific Ocean lies to the east while the S...                   66360   \n",
      "2  Probiotics are found in foods such as yogurt, ...                   88507   \n",
      "3  Iodine is critical to thyroid health and funct...                   87550   \n",
      "4  The answer to the question how much does it co...                   61479   \n",
      "\n",
      "                                           query_emb  query_length  \\\n",
      "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
      "1  [[tensor(-0.1313), tensor(-0.4520), tensor(0.0...             5   \n",
      "2  [[tensor(0.2753), tensor(0.2256), tensor(-0.29...            10   \n",
      "3  [[tensor(0.4291), tensor(-0.1227), tensor(0.05...             6   \n",
      "4  [[tensor(0.1736), tensor(0.3796), tensor(0.132...             5   \n",
      "\n",
      "                                             pos_emb  pos_length  \\\n",
      "0  [[tensor(-0.0312), tensor(-0.0221), tensor(0.0...         110   \n",
      "1  [[tensor(0.1288), tensor(-0.8221), tensor(0.27...         104   \n",
      "2  [[tensor(0.1867), tensor(-0.7999), tensor(0.79...         101   \n",
      "3  [[tensor(0.0911), tensor(-0.3988), tensor(0.30...          53   \n",
      "4  [[tensor(-0.0087), tensor(0.0675), tensor(-0.1...          49   \n",
      "\n",
      "                                             neg_emb  neg_length  \n",
      "0  [[tensor(0.0302), tensor(0.4461), tensor(0.431...          67  \n",
      "1  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          95  \n",
      "2  [[tensor(-0.3277), tensor(-0.4549), tensor(-0....          98  \n",
      "3  [[tensor(-0.2682), tensor(-0.0228), tensor(0.2...          49  \n",
      "4  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          46  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_soft_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_soft_neg_ext = pd.concat([df_soft_neg, embeddings_padded], axis=1)\n",
    "print(df_soft_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_soft_neg_ext['query_pos_sim'] > df_soft_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_soft_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1926f19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>positive_passage</th>\n",
       "      <th>negative_passage</th>\n",
       "      <th>negative_from_query_id</th>\n",
       "      <th>query_emb</th>\n",
       "      <th>query_length</th>\n",
       "      <th>pos_emb</th>\n",
       "      <th>pos_length</th>\n",
       "      <th>neg_emb</th>\n",
       "      <th>neg_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19699</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>Results-Based Accountability® (also known as R...</td>\n",
       "      <td>I finally found some real salary data for phys...</td>\n",
       "      <td>86595</td>\n",
       "      <td>[[tensor(0.0424), tensor(-0.5220), tensor(0.40...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[tensor(-0.0312), tensor(-0.0221), tensor(0.0...</td>\n",
       "      <td>110</td>\n",
       "      <td>[[tensor(0.0302), tensor(0.4461), tensor(0.431...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id        query                                   positive_passage  \\\n",
       "0     19699  what is rba  Results-Based Accountability® (also known as R...   \n",
       "\n",
       "                                    negative_passage  negative_from_query_id  \\\n",
       "0  I finally found some real salary data for phys...                   86595   \n",
       "\n",
       "                                           query_emb  query_length  \\\n",
       "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
       "\n",
       "                                             pos_emb  pos_length  \\\n",
       "0  [[tensor(-0.0312), tensor(-0.0221), tensor(0.0...         110   \n",
       "\n",
       "                                             neg_emb  neg_length  \n",
       "0  [[tensor(0.0302), tensor(0.4461), tensor(0.431...          67  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_soft_neg_ext[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fbbd3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings... This may take a while depending on dataframe size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79700/79700 [02:35<00:00, 512.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id                                              query  \\\n",
      "0     19699                                        what is rba   \n",
      "1     19700                       was ronald reagan a democrat   \n",
      "2     19701  how long do you need for sydney and surroundin...   \n",
      "3     19702                    price to install tile in shower   \n",
      "4     19703                    why conversion observed in body   \n",
      "\n",
      "                                    positive_passage  \\\n",
      "0  Results-Based Accountability® (also known as R...   \n",
      "1  From Wikipedia, the free encyclopedia. A Reaga...   \n",
      "2  Sydney is the capital city of the Australian s...   \n",
      "3  1 Install ceramic tile floor to match shower-A...   \n",
      "4  Conversion disorder is a type of somatoform di...   \n",
      "\n",
      "                                    negative_passage  negative_index_in_group  \\\n",
      "0  vs. NetIQ Identity Manager. Risk-based authent...                        8   \n",
      "1  1984 Re-Election. In November 1984, Ronald Rea...                        7   \n",
      "2  The Sydney central business district, Sydney h...                        3   \n",
      "3  The national average for a new shower installa...                        8   \n",
      "4  Conclusions: In adult body CT, dose to an orga...                        1   \n",
      "\n",
      "                                           query_emb  query_length  \\\n",
      "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
      "1  [[tensor(-0.1313), tensor(-0.4520), tensor(0.0...             5   \n",
      "2  [[tensor(0.2753), tensor(0.2256), tensor(-0.29...            10   \n",
      "3  [[tensor(0.4291), tensor(-0.1227), tensor(0.05...             6   \n",
      "4  [[tensor(0.1736), tensor(0.3796), tensor(0.132...             5   \n",
      "\n",
      "                                             pos_emb  pos_length  \\\n",
      "0  [[tensor(-0.0395), tensor(0.0841), tensor(0.09...         110   \n",
      "1  [[tensor(0.1288), tensor(-0.8221), tensor(0.27...         104   \n",
      "2  [[tensor(0.1867), tensor(-0.7999), tensor(0.79...         101   \n",
      "3  [[tensor(0.0911), tensor(-0.3988), tensor(0.30...          53   \n",
      "4  [[tensor(-0.0087), tensor(0.0675), tensor(-0.1...          49   \n",
      "\n",
      "                                             neg_emb  neg_length  \n",
      "0  [[tensor(0.2484), tensor(0.2407), tensor(0.388...          81  \n",
      "1  [[tensor(-0.1478), tensor(0.3440), tensor(-0.1...          53  \n",
      "2  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...         100  \n",
      "3  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          67  \n",
      "4  [[tensor(-0.5504), tensor(0.1625), tensor(0.51...          86  \n"
     ]
    }
   ],
   "source": [
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_hard_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_hard_neg_ext = pd.concat([df_hard_neg, embeddings_padded], axis=1)\n",
    "print(df_hard_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_hard_neg_ext['query_pos_sim'] > df_hard_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_hard_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13cea888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>positive_passage</th>\n",
       "      <th>negative_passage</th>\n",
       "      <th>negative_from_query_id</th>\n",
       "      <th>query_emb</th>\n",
       "      <th>query_length</th>\n",
       "      <th>pos_emb</th>\n",
       "      <th>pos_length</th>\n",
       "      <th>neg_emb</th>\n",
       "      <th>neg_length</th>\n",
       "      <th>negative_index_in_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19699</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>Results-Based Accountability® (also known as R...</td>\n",
       "      <td>I finally found some real salary data for phys...</td>\n",
       "      <td>86595.0</td>\n",
       "      <td>[[tensor(0.0424), tensor(-0.5220), tensor(0.40...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[tensor(-0.0312), tensor(-0.0221), tensor(0.0...</td>\n",
       "      <td>110</td>\n",
       "      <td>[[tensor(0.0302), tensor(0.4461), tensor(0.431...</td>\n",
       "      <td>67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19700</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>From Wikipedia, the free encyclopedia. A Reaga...</td>\n",
       "      <td>The Pacific Ocean lies to the east while the S...</td>\n",
       "      <td>66360.0</td>\n",
       "      <td>[[tensor(-0.1313), tensor(-0.4520), tensor(0.0...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[tensor(0.1288), tensor(-0.8221), tensor(0.27...</td>\n",
       "      <td>104</td>\n",
       "      <td>[[tensor(-0.1077), tensor(0.1105), tensor(0.59...</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19701</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>Sydney is the capital city of the Australian s...</td>\n",
       "      <td>Probiotics are found in foods such as yogurt, ...</td>\n",
       "      <td>88507.0</td>\n",
       "      <td>[[tensor(0.2753), tensor(0.2256), tensor(-0.29...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[tensor(0.1867), tensor(-0.7999), tensor(0.79...</td>\n",
       "      <td>101</td>\n",
       "      <td>[[tensor(-0.3277), tensor(-0.4549), tensor(-0....</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19702</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>1 Install ceramic tile floor to match shower-A...</td>\n",
       "      <td>Iodine is critical to thyroid health and funct...</td>\n",
       "      <td>87550.0</td>\n",
       "      <td>[[tensor(0.4291), tensor(-0.1227), tensor(0.05...</td>\n",
       "      <td>6</td>\n",
       "      <td>[[tensor(0.0911), tensor(-0.3988), tensor(0.30...</td>\n",
       "      <td>53</td>\n",
       "      <td>[[tensor(-0.2682), tensor(-0.0228), tensor(0.2...</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19703</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>Conversion disorder is a type of somatoform di...</td>\n",
       "      <td>The answer to the question how much does it co...</td>\n",
       "      <td>61479.0</td>\n",
       "      <td>[[tensor(0.1736), tensor(0.3796), tensor(0.132...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[tensor(-0.0087), tensor(0.0675), tensor(-0.1...</td>\n",
       "      <td>49</td>\n",
       "      <td>[[tensor(-0.1077), tensor(0.1105), tensor(0.59...</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                              query  \\\n",
       "0     19699                                        what is rba   \n",
       "1     19700                       was ronald reagan a democrat   \n",
       "2     19701  how long do you need for sydney and surroundin...   \n",
       "3     19702                    price to install tile in shower   \n",
       "4     19703                    why conversion observed in body   \n",
       "\n",
       "                                    positive_passage  \\\n",
       "0  Results-Based Accountability® (also known as R...   \n",
       "1  From Wikipedia, the free encyclopedia. A Reaga...   \n",
       "2  Sydney is the capital city of the Australian s...   \n",
       "3  1 Install ceramic tile floor to match shower-A...   \n",
       "4  Conversion disorder is a type of somatoform di...   \n",
       "\n",
       "                                    negative_passage  negative_from_query_id  \\\n",
       "0  I finally found some real salary data for phys...                 86595.0   \n",
       "1  The Pacific Ocean lies to the east while the S...                 66360.0   \n",
       "2  Probiotics are found in foods such as yogurt, ...                 88507.0   \n",
       "3  Iodine is critical to thyroid health and funct...                 87550.0   \n",
       "4  The answer to the question how much does it co...                 61479.0   \n",
       "\n",
       "                                           query_emb  query_length  \\\n",
       "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
       "1  [[tensor(-0.1313), tensor(-0.4520), tensor(0.0...             5   \n",
       "2  [[tensor(0.2753), tensor(0.2256), tensor(-0.29...            10   \n",
       "3  [[tensor(0.4291), tensor(-0.1227), tensor(0.05...             6   \n",
       "4  [[tensor(0.1736), tensor(0.3796), tensor(0.132...             5   \n",
       "\n",
       "                                             pos_emb  pos_length  \\\n",
       "0  [[tensor(-0.0312), tensor(-0.0221), tensor(0.0...         110   \n",
       "1  [[tensor(0.1288), tensor(-0.8221), tensor(0.27...         104   \n",
       "2  [[tensor(0.1867), tensor(-0.7999), tensor(0.79...         101   \n",
       "3  [[tensor(0.0911), tensor(-0.3988), tensor(0.30...          53   \n",
       "4  [[tensor(-0.0087), tensor(0.0675), tensor(-0.1...          49   \n",
       "\n",
       "                                             neg_emb  neg_length  \\\n",
       "0  [[tensor(0.0302), tensor(0.4461), tensor(0.431...          67   \n",
       "1  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          95   \n",
       "2  [[tensor(-0.3277), tensor(-0.4549), tensor(-0....          98   \n",
       "3  [[tensor(-0.2682), tensor(-0.0228), tensor(0.2...          49   \n",
       "4  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          46   \n",
       "\n",
       "   negative_index_in_group  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_neg_ext = pd.concat([df_soft_neg_ext, df_hard_neg_ext])\n",
    "df_all_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766f3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to pickle format\n",
    "df_soft_neg_ext.to_pickle(\"downloaded_model/df_soft_neg_ext.pkl\")\n",
    "df_hard_neg_ext.to_pickle(\"downloaded_model/df_hard_neg_ext.pkl\")\n",
    "df_all_neg_ext.to_pickle(\"downloaded_model/df_all_neg_ext.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02765d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a DataFrame from pickle if the file exists\n",
    "def load_df_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return pd.read_pickle(file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Load DataFrames\n",
    "df_soft_neg_ext = load_df_if_exists(\"data/df_soft_neg_ext.pkl\")\n",
    "df_hard_neg_ext = load_df_if_exists(\"data/df_hard_neg_ext.pkl\")\n",
    "df_all_neg_ext = load_df_if_exists(\"data/df_all_neg_ext.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create packed sequences for RNN processing\n",
    "packed_queries, packed_positives, packed_negatives = create_packed_batch(df_all_neg_ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd683001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feed packed sequences to your RNN models\n",
    "query_outputs, query_hidden =your_query_rnn(packed_queries)\n",
    "pos_outputs, pos_hidden = your_document_rnn(packed_positives)\n",
    "neg_outputs, neg_hidden = your_document_rnn(packed_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hyperparameter_tuning(df_all_neg_ext, output_dims=[100], batch_sizes=[512], gru_hidden_dims=[100], \n",
    "                         num_layers=[1], dropouts=[0.1], learning_rates=[1e-3], \n",
    "                         epochs=1, log_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3edcd",
   "metadata": {},
   "source": [
    "### Twin Tower Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tower instances\n",
    "#qryTower = QryTower()\n",
    "#docTower = DocTower()\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 128\n",
    "num_epochs = 1 # adjust num of epochs here\n",
    "dataset_size = len(df_all_neg_ext)  # or len(df_hard_neg_ext) depending on the dataset you want to use\n",
    "steps_per_epoch = dataset_size // batch_size\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "learning_rate = 1e-3\n",
    "embedding_dim = 100 #changed for glove \n",
    "margin = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = TripletEmbeddingDataset(df_all_neg_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7753f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    #num_workers=2,  # Adjust based on your machine's capabilities\n",
    "    pin_memory=True  # Set to True if using GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = torch.randn(batch_size, embedding_dim)  # Query embeddings\n",
    "pos = torch.randn(batch_size, embedding_dim)  # Positive doc embeddings\n",
    "neg = torch.randn(batch_size, embedding_dim)  # Negative doc embeddings\n",
    "\n",
    "#qry = df1['q']\n",
    "\n",
    "\n",
    "# Set up the AdamW optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': qryTower.parameters()},\n",
    "    {'params': docTower.parameters()}\n",
    "], lr=learning_rate)\n",
    "\n",
    "# Add learning rate scheduler (ReduceLROnPlateau)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',       # Reduce LR when monitored value stops decreasing\n",
    "    factor=0.5,       # Multiply LR by this factor when reducing\n",
    "    patience=2,       # Number of epochs with no improvement after which LR will be reduced\n",
    "    verbose=True      # Print message when LR is reduced\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (simplified example)\n",
    "for epoch in range(num_epochs):\n",
    "    qryTower.train()\n",
    "    docTower.train()\n",
    "    \n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Get embeddings from batch\n",
    "        query_emb = batch['query']\n",
    "        pos_emb = batch['positive']\n",
    "        neg_emb = batch['negative']\n",
    "        \n",
    "        # Forward pass through towers\n",
    "        query_encoded = qryTower(query_emb)\n",
    "        pos_encoded = docTower(pos_emb)\n",
    "        neg_encoded = docTower(neg_emb)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_sim = torch.nn.functional.cosine_similarity(query_encoded, pos_encoded)\n",
    "        neg_sim = torch.nn.functional.cosine_similarity(query_encoded, neg_encoded)\n",
    "        \n",
    "        # Triplet loss\n",
    "        margin = margin\n",
    "        loss = torch.clamp(margin - pos_sim + neg_sim, min=0).mean()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(query_emb)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter tuning with your dataframe\n",
    "best_params, final_qry_tower, final_doc_tower = run_hyperparameter_tuning(\n",
    "    df_all_neg_ext,\n",
    "    output_dims=[100],\n",
    "    batch_sizes=[512],\n",
    "    epochs=7\n",
    ")\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best output dimension: {best_params['output_dim']}\")\n",
    "print(f\"Best batch size: {best_params['batch_size']}\")\n",
    "print(f\"Best validation loss: {best_params['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(qryTower, docTower, dataloader, device):\n",
    "    qryTower.eval()\n",
    "    docTower.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get embeddings from batch\n",
    "        query_emb = batch['query']\n",
    "        pos_emb = batch['positive']\n",
    "        neg_emb = batch['negative']\n",
    "        \n",
    "        # Forward pass through towers\n",
    "        query_encoded = qryTower(query_emb)\n",
    "        pos_encoded = docTower(pos_emb)\n",
    "        neg_encoded = docTower(neg_emb)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_sim = torch.nn.functional.cosine_similarity(query_encoded, pos_encoded)\n",
    "        neg_sim = torch.nn.functional.cosine_similarity(query_encoded, neg_encoded)\n",
    "\n",
    "        correct += (pos_sim > neg_sim).sum().item()\n",
    "        total += batch['query'].size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Eval Accuracy (query closer to pos than neg): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53907333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(dataloader):.4f}\")\n",
    "evaluate_model(final_qry_tower, final_doc_tower, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "pos_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "\n",
    "# Ensure tensors have at least two dimensions before applying mean\n",
    "if query_emb.dim() == 1:\n",
    "\tquery_emb = query_emb.unsqueeze(0)\n",
    "if pos_emb.dim() == 1:\n",
    "\tpos_emb = pos_emb.unsqueeze(0)\n",
    "\n",
    "query_emb = query_emb.mean(dim=0)\n",
    "pos_emb = pos_emb.mean(dim=0)\n",
    "\n",
    "print(torch.nn.functional.cosine_similarity(query_emb, pos_emb, dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_qry_tower.eval()\n",
    "final_doc_tower.eval()\n",
    "query_encoded = final_qry_tower(query_emb)\n",
    "pos_encoded = final_doc_tower(pos_emb)\n",
    "torch.nn.functional.cosine_similarity(query_encoded, pos_encoded, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11581eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload final model to wandb\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load your API key from config.txt\n",
    "def load_api_key_from_config(config_path=\"config.txt\"):\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "            if \"=\" in first_line:\n",
    "                api_key = first_line.split(\"=\")[1].strip()\n",
    "            else:\n",
    "                api_key = first_line\n",
    "        return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Config file not found at {config_path}\")\n",
    "        return None\n",
    "\n",
    "# Set up wandb - only set API key if wandb.run doesn't exist yet\n",
    "if wandb.run is None:\n",
    "    api_key = load_api_key_from_config()\n",
    "    if api_key:\n",
    "        os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "        wandb.login()\n",
    "        print(\"Successfully logged in to Weights & Biases\")\n",
    "    else:\n",
    "        print(\"Failed to load API key\")\n",
    "\n",
    "# Check if there's an active run, only initialize if needed\n",
    "if wandb.run is None:\n",
    "    run = wandb.init(\n",
    "        project=\"twin-tower-model\",\n",
    "        name=\"final-model-summary\",  # Changed name to indicate this is a summary\n",
    "        config={\n",
    "            \"output_dim\": best_params[\"output_dim\"],\n",
    "            \"batch_size\": best_params[\"batch_size\"],\n",
    "            \"architecture\": \"Twin Tower Network\",\n",
    "            \"dataset\": \"MS MARCO\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    run = wandb.run\n",
    "    # Update the run with additional metadata if needed\n",
    "    run.config.update({\n",
    "        \"architecture\": \"Twin Tower Network\",\n",
    "        \"dataset\": \"MS MARCO\"\n",
    "    })\n",
    "\n",
    "# Upload the model with a timestamp to avoid conflicts\n",
    "timestamp2 = int(time.time())\n",
    "artifact_name = f\"twin-tower-final-model-{timestamp2}\"\n",
    "\n",
    "model_artifact = wandb.Artifact(\n",
    "    name=artifact_name, \n",
    "    type=\"model\",\n",
    "    description=\"Twin Tower model trained on full training data with optimal hyperparameters\"\n",
    ")\n",
    "\n",
    "final_model_path = f\"checkpoints/final_model/final_model.pt\"\n",
    "model_artifact.add_file(final_model_path)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "print(f\"Final model uploaded to Weights & Biases project: {run.project}\")\n",
    "\n",
    "# Only finish the run if we created it in this script\n",
    "if run.name == \"final-model-summary\":\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

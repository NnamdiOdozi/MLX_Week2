{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b0ec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnamdi-odozi\u001b[0m (\u001b[33mnnamdi-odozi-ave-actuaries\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from load_models_and_data import load_vocabulary, load_embeddings, text_to_embeddings, calc_cosine_sim, calculate_embeddings, create_packed_batch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#from TwoTowerNN import QryTower, DocTower, TripletEmbeddingDataset, run_hyperparameter_tuning\n",
    "from TwinTowerGRU import QryTower, DocTower, EmbeddingTripletDataset, run_hyperparameter_tuning, GRUTwinTowerModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader,  SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b860fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba466e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading datasets from Hugging Face\n",
    "ds_soft_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_soft_negatives\")\n",
    "#ds_hard_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1948ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and vocabulary...\n",
      "Loaded embeddings with shape: torch.Size([400000, 100])\n",
      "Loaded vocabulary with 399998 tokens\n",
      "Embedded text shape: torch.Size([26, 100])\n",
      "Embedding array with custom formatting:\n",
      "[[ 0.2616  0.4472 -0.0968 ... -0.4503  0.4952 -0.203 ]\n",
      " [ 0.1372 -0.5429  0.1942 ... -0.5206  0.2543 -0.2376]\n",
      " [-0.3046 -0.2365  0.1758 ... -0.8456 -0.0354  0.1704]\n",
      " ...\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]]\n",
      "Length is: 5\n"
     ]
    }
   ],
   "source": [
    "# Paths to your files\n",
    "embeddings_path = \"./downloaded_model/glove_embeddings.pt\" #set this to either own-trained cbow ones or to glove pre-trained ones\n",
    "vocab_path = \"./downloaded_model/glove_ids_to_words.csv\"\n",
    "\n",
    "# Load embeddings and vocabulary\n",
    "print(\"Loading embeddings and vocabulary...\")\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "word_to_idx = load_vocabulary(vocab_path)\n",
    "\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"Loaded vocabulary with {len(word_to_idx)} tokens\")\n",
    "\n",
    "# Example usage (uncomment when ready to test)\n",
    "sample_text = \"This is a test sentence\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "# Testing - Set numpy print options\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff426027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded text shape: torch.Size([26, 100])\n",
      "Embedding array with custom formatting:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Length is: 0\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a28244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg  = pd.DataFrame(ds_soft_neg['train'])\n",
    "#df_hard_neg  = pd.DataFrame(ds_hard_neg['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b81dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query, length = text_to_embeddings(df_soft_neg['query'][0], word_to_idx, embeddings, is_query=True)\n",
    "embedded_positive, length = text_to_embeddings(df_soft_neg['positive_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "embedded_negative, length = text_to_embeddings(df_soft_neg['negative_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "\n",
    "print(embedded_positive.shape)\n",
    "print(embedded_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedded_query.mean(dim=0)\n",
    "b = embedded_positive.mean(dim=0)\n",
    "c = embedded_negative.mean(dim=0)\n",
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cosine_similarity = F.cosine_similarity(a, c, dim=0)\n",
    "print(f\"Cosine similarity between query and positive passage: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Process the dataframe using apply just for first five rows\n",
    "# print(\"Calculating similarities... This may take a while depending on dataframe size.\")\n",
    "# similarities = df_soft_neg[0:5].progress_apply(\n",
    "#     lambda row: calculate_similarities(row, word_to_idx, embeddings), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Join the similarities to the dataframe\n",
    "# df_soft_neg_ext = pd.concat([df_soft_neg[0:5], similarities], axis=1)\n",
    "\n",
    "# # Show a sample of the results\n",
    "# #print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "#print(df_soft_neg_ext.head())\n",
    "#print(df_soft_neg_ext.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a02f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings... This may take a while depending on dataframe size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79704/79704 [02:46<00:00, 479.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id                                              query  \\\n",
      "0     19699                                        what is rba   \n",
      "1     19700                       was ronald reagan a democrat   \n",
      "2     19701  how long do you need for sydney and surroundin...   \n",
      "3     19702                    price to install tile in shower   \n",
      "4     19703                    why conversion observed in body   \n",
      "\n",
      "                                    positive_passage  \\\n",
      "0  Results-Based Accountability® (also known as R...   \n",
      "1  From Wikipedia, the free encyclopedia. A Reaga...   \n",
      "2  Sydney is the capital city of the Australian s...   \n",
      "3  1 Install ceramic tile floor to match shower-A...   \n",
      "4  Conversion disorder is a type of somatoform di...   \n",
      "\n",
      "                                    negative_passage  negative_from_query_id  \\\n",
      "0  I finally found some real salary data for phys...                   86595   \n",
      "1  The Pacific Ocean lies to the east while the S...                   66360   \n",
      "2  Probiotics are found in foods such as yogurt, ...                   88507   \n",
      "3  Iodine is critical to thyroid health and funct...                   87550   \n",
      "4  The answer to the question how much does it co...                   61479   \n",
      "\n",
      "                                           query_emb  query_length  \\\n",
      "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
      "1  [[tensor(-0.1313), tensor(-0.4520), tensor(0.0...             5   \n",
      "2  [[tensor(0.2753), tensor(0.2256), tensor(-0.29...            10   \n",
      "3  [[tensor(0.4291), tensor(-0.1227), tensor(0.05...             6   \n",
      "4  [[tensor(0.1736), tensor(0.3796), tensor(0.132...             5   \n",
      "\n",
      "                                             pos_emb  pos_length  \\\n",
      "0  [[tensor(-0.2441), tensor(-0.1118), tensor(0.0...         110   \n",
      "1  [[tensor(0.1288), tensor(-0.8221), tensor(0.27...         104   \n",
      "2  [[tensor(0.1867), tensor(-0.7999), tensor(0.79...         101   \n",
      "3  [[tensor(0.0911), tensor(-0.3988), tensor(0.30...          53   \n",
      "4  [[tensor(-0.0087), tensor(0.0675), tensor(-0.1...          49   \n",
      "\n",
      "                                             neg_emb  neg_length  \n",
      "0  [[tensor(0.0302), tensor(0.4461), tensor(0.431...          67  \n",
      "1  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          95  \n",
      "2  [[tensor(-0.3277), tensor(-0.4549), tensor(-0....          98  \n",
      "3  [[tensor(-0.2682), tensor(-0.0228), tensor(0.2...          49  \n",
      "4  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          46  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_soft_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_soft_neg_ext = pd.concat([df_soft_neg, embeddings_padded], axis=1)\n",
    "print(df_soft_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_soft_neg_ext['query_pos_sim'] > df_soft_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_soft_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg_ext[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_hard_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_hard_neg_ext = pd.concat([df_hard_neg, embeddings_padded], axis=1)\n",
    "print(df_hard_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_hard_neg_ext['query_pos_sim'] > df_hard_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_hard_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cea888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_neg_ext = pd.concat([df_soft_neg_ext, df_hard_neg_ext])\n",
    "df_all_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766f3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to pickle format\n",
    "df_soft_neg_ext.to_pickle(\"downloaded_model/df_soft_neg_ext.pkl\")\n",
    "#df_hard_neg_ext.to_pickle(\"downloaded_model/df_hard_neg_ext.pkl\")\n",
    "#df_all_neg_ext.to_pickle(\"downloaded_model/df_all_neg_ext.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02765d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x00'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load DataFrames\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_soft_neg_ext \u001b[38;5;241m=\u001b[39m \u001b[43mload_df_if_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdownloaded_model/df_soft_neg_ext.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#df_hard_neg_ext = load_df_if_exists(\"downloaded_model/df_hard_neg_ext.pkl\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#df_all_neg_ext = load_df_if_exists(\"downloaded_model/df_all_neg_ext.pkl\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mload_df_if_exists\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_df_if_exists\u001b[39m(file_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/pandas/io/pickle.py:202\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'."
     ]
    }
   ],
   "source": [
    "# Function to load a DataFrame from pickle if the file exists\n",
    "def load_df_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return pd.read_pickle(file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Load DataFrames\n",
    "df_soft_neg_ext = load_df_if_exists(\"downloaded_model/df_soft_neg_ext.pkl\")\n",
    "#df_hard_neg_ext = load_df_if_exists(\"downloaded_model/df_hard_neg_ext.pkl\")\n",
    "#df_all_neg_ext = load_df_if_exists(\"downloaded_model/df_all_neg_ext.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create packed sequences for RNN processing\n",
    "#packed_queries, packed_positives, packed_negatives = create_packed_batch(df_all_neg_ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd683001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Feed packed sequences to your RNN models\n",
    "# query_outputs, query_hidden =your_query_rnn(packed_queries)\n",
    "# pos_outputs, pos_hidden = your_document_rnn(packed_positives)\n",
    "# neg_outputs, neg_hidden = your_document_rnn(packed_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hyperparameter_tuning(df_soft_neg_ext, output_dims=[100], batch_sizes=[512, 1024], gru_hidden_dims=[100,200], \n",
    "                         num_layers=[1], dropouts=[0.1], learning_rates=[1e-3], \n",
    "                         epochs=10, log_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3edcd",
   "metadata": {},
   "source": [
    "### Twin Tower Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05391efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: checkpoints/final_gru_model_20250424-152045/final_gru_model_20250424-152045.pt\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"checkpoints\", \"final_gru_model_20250424-152045\", \"final_gru_model_20250424-152045.pt\")\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model instance\n",
    "model = GRUTwinTowerModel(embedding_dim=100, gru_hidden_dim=100, output_dim=100, \n",
    "                         num_layers=1, dropout=0.1)\n",
    "\n",
    "# Load the checkpoint and extract the model state dict\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "# The error shows the state_dict is nested under \"model_state_dict\"\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b981b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model at: /root/MLX_Week2/artifacts/final_gru_model_20250424-174424:v0/final_gru_model_20250424-174424.pt\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained model from WandB\n",
    "#run = wandb.init()\n",
    "# The correct artifact path format\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(\"nnamdi-odozi-ave-actuaries/gru-twin-tower-model/final_gru_model_20250424-174424:v0\")\n",
    "\n",
    "#https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/artifacts/model/final_gru_model_20250424-174424/v0/files/final_gru_model_20250424-174424.pt\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Find the model file\n",
    "import os\n",
    "model_files = [f for f in os.listdir(artifact_dir) if f.endswith('.pt') or f.endswith('.pth')]\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"No model files found in {artifact_dir}\")\n",
    "\n",
    "model_path = os.path.join(artifact_dir, model_files[0])\n",
    "print(f\"Found model at: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Create model with correct dimensions\n",
    "model2 = GRUTwinTowerModel(\n",
    "    embedding_dim=100, \n",
    "    gru_hidden_dim=100,  # Use 100 as seen in your model print\n",
    "    output_dim=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Load state dict (handle both formats)\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    model2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    model2.load_state_dict(checkpoint)\n",
    "\n",
    "model2 = model.to(device)\n",
    "model2.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8d5f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUTwinTowerModel(\n",
      "  (query_encoder): BidirectionalGRU(\n",
      "    (gru): GRU(100, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (doc_encoder): BidirectionalGRU(\n",
      "    (gru): GRU(100, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (query_tower): QryTower(\n",
      "    (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (doc_tower): DocTower(\n",
      "    (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8847a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: [0.0583 0.0494 0.0938]\n"
     ]
    }
   ],
   "source": [
    "# 2. Test with consecutive rows - just use a slice directly\n",
    "df_slice = df_soft_neg_ext[0:3]  # Use any 3 consecutive rows\n",
    "\n",
    "# Process dataframe slice\n",
    "with torch.no_grad():\n",
    "    # Move everything to device\n",
    "    query_embs = torch.stack(df_slice['query_emb'].tolist()).to(device)\n",
    "    query_lens = torch.tensor(df_slice['query_length'].tolist()).to(device)\n",
    "    pos_embs = torch.stack(df_slice['pos_emb'].tolist()).to(device)\n",
    "    pos_lens = torch.tensor(df_slice['pos_length'].tolist()).to(device)\n",
    "    \n",
    "    # Get encodings for all rows at once\n",
    "    query_vecs, doc_vecs = model(query_embs, query_lens, pos_embs, pos_lens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = torch.nn.functional.cosine_similarity(query_vecs, doc_vecs, dim=1)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b10fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>positive_passage</th>\n",
       "      <th>negative_passage</th>\n",
       "      <th>negative_from_query_id</th>\n",
       "      <th>query_emb</th>\n",
       "      <th>query_length</th>\n",
       "      <th>pos_emb</th>\n",
       "      <th>pos_length</th>\n",
       "      <th>neg_emb</th>\n",
       "      <th>neg_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19699</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>Results-Based Accountability® (also known as R...</td>\n",
       "      <td>I finally found some real salary data for phys...</td>\n",
       "      <td>86595</td>\n",
       "      <td>[[tensor(0.0424), tensor(-0.5220), tensor(0.40...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[tensor(-0.2441), tensor(-0.1118), tensor(0.0...</td>\n",
       "      <td>110</td>\n",
       "      <td>[[tensor(0.0302), tensor(0.4461), tensor(0.431...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19700</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>From Wikipedia, the free encyclopedia. A Reaga...</td>\n",
       "      <td>The Pacific Ocean lies to the east while the S...</td>\n",
       "      <td>66360</td>\n",
       "      <td>[[tensor(-0.1313), tensor(-0.4520), tensor(0.0...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[tensor(0.1288), tensor(-0.8221), tensor(0.27...</td>\n",
       "      <td>104</td>\n",
       "      <td>[[tensor(-0.1077), tensor(0.1105), tensor(0.59...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19701</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>Sydney is the capital city of the Australian s...</td>\n",
       "      <td>Probiotics are found in foods such as yogurt, ...</td>\n",
       "      <td>88507</td>\n",
       "      <td>[[tensor(0.2753), tensor(0.2256), tensor(-0.29...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[tensor(0.1867), tensor(-0.7999), tensor(0.79...</td>\n",
       "      <td>101</td>\n",
       "      <td>[[tensor(-0.3277), tensor(-0.4549), tensor(-0....</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                              query  \\\n",
       "0     19699                                        what is rba   \n",
       "1     19700                       was ronald reagan a democrat   \n",
       "2     19701  how long do you need for sydney and surroundin...   \n",
       "\n",
       "                                    positive_passage  \\\n",
       "0  Results-Based Accountability® (also known as R...   \n",
       "1  From Wikipedia, the free encyclopedia. A Reaga...   \n",
       "2  Sydney is the capital city of the Australian s...   \n",
       "\n",
       "                                    negative_passage  negative_from_query_id  \\\n",
       "0  I finally found some real salary data for phys...                   86595   \n",
       "1  The Pacific Ocean lies to the east while the S...                   66360   \n",
       "2  Probiotics are found in foods such as yogurt, ...                   88507   \n",
       "\n",
       "                                           query_emb  query_length  \\\n",
       "0  [[tensor(0.0424), tensor(-0.5220), tensor(0.40...             3   \n",
       "1  [[tensor(-0.1313), tensor(-0.4520), tensor(0.0...             5   \n",
       "2  [[tensor(0.2753), tensor(0.2256), tensor(-0.29...            10   \n",
       "\n",
       "                                             pos_emb  pos_length  \\\n",
       "0  [[tensor(-0.2441), tensor(-0.1118), tensor(0.0...         110   \n",
       "1  [[tensor(0.1288), tensor(-0.8221), tensor(0.27...         104   \n",
       "2  [[tensor(0.1867), tensor(-0.7999), tensor(0.79...         101   \n",
       "\n",
       "                                             neg_emb  neg_length  \n",
       "0  [[tensor(0.0302), tensor(0.4461), tensor(0.431...          67  \n",
       "1  [[tensor(-0.1077), tensor(0.1105), tensor(0.59...          95  \n",
       "2  [[tensor(-0.3277), tensor(-0.4549), tensor(-0....          98  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1e2c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: [0.0583 0.0494 0.0938 ... 0.11   0.0591 0.1024]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07484391"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Test with consecutive rows - just use a slice directly\n",
    "df_slice = df_soft_neg_ext[0:512]  # Doing more rows\n",
    "\n",
    "# Process dataframe slice\n",
    "with torch.no_grad():\n",
    "    # Move everything to device\n",
    "    query_embs = torch.stack(df_slice['query_emb'].tolist()).to(device)\n",
    "    query_lens = torch.tensor(df_slice['query_length'].tolist()).to(device)\n",
    "    pos_embs = torch.stack(df_slice['pos_emb'].tolist()).to(device)\n",
    "    pos_lens = torch.tensor(df_slice['pos_length'].tolist()).to(device)\n",
    "    \n",
    "    # Get encodings for all rows at once\n",
    "    query_vecs, doc_vecs = model(query_embs, query_lens, pos_embs, pos_lens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = torch.nn.functional.cosine_similarity(query_vecs, doc_vecs, dim=1)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())\n",
    "sims.cpu().numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc93caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUTwinTowerModel(\n",
      "  (query_encoder): BidirectionalGRU(\n",
      "    (gru): GRU(100, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (doc_encoder): BidirectionalGRU(\n",
      "    (gru): GRU(100, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (query_tower): QryTower(\n",
      "    (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (doc_tower): DocTower(\n",
      "    (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd960918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with a random sentence:\n",
    "query_test = \"This is RBA\"\n",
    "doc_test = \"This is RBA\"\n",
    "q_l = len(query_test.split())\n",
    "d_l = len(doc_test.split())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87f984ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 100])\n",
      "tensor([[ 0.2616,  0.4472, -0.0968,  ..., -0.4503,  0.4952, -0.2030],\n",
      "        [ 0.1372, -0.5429,  0.1942,  ..., -0.5206,  0.2543, -0.2376],\n",
      "        [ 0.7096, -0.3907, -0.7100,  ...,  0.1420, -1.2771,  0.4431],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "query_emb, q_l = text_to_embeddings(query_test, word_to_idx, embeddings, is_query=True)\n",
    "doc_emb, d_l = text_to_embeddings(doc_test, word_to_idx, embeddings, is_query=False)\n",
    "print(query_emb.shape)\n",
    "print(doc_emb)\n",
    "print(q_l, d_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64612c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_emb.mean(dim=0)\n",
    "d = doc_emb.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43464337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarities\n",
    "sims = torch.nn.functional.cosine_similarity(q, d, dim=0)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab178558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make sure tensors are on the right device\n",
    "device = next(model.parameters()).device\n",
    "query_emb = query_emb.to(device)  # Shape should be [seq_length, embedding_dim]\n",
    "q_l = torch.tensor([q_l], device=device)  # Single value for sequence length\n",
    "\n",
    "# 2. Add batch dimension for model processing\n",
    "query_emb = query_emb.unsqueeze(0)  # Shape becomes [1, seq_length, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "871e3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector shape: torch.Size([1, 100])\n",
      "Values: [-0.0142  0.1359 -0.0488  0.0435  0.1144]\n"
     ]
    }
   ],
   "source": [
    "# 3. Query-only inference using just dataframe columns \n",
    "#query_row = df_soft_neg_ext[0]  # Use any row\n",
    "#test_query_emb = query_row['query_emb'].unsqueeze(0).to(device)\n",
    "#test_query_len = torch.tensor([query_row['query_length']]).to(device)\n",
    "\n",
    "# Just run through query encoder and tower\n",
    "with torch.no_grad():\n",
    "    query_encoded = model.query_encoder(query_emb, q_l)\n",
    "    query_vector = model.query_tower(query_encoded)\n",
    "    query_vector = torch.nn.functional.normalize(query_vector, p=2, dim=1) #I don't think this is needed, but let's keep it for now\n",
    "\n",
    "print(\"Query vector shape:\", query_vector.shape)\n",
    "print(\"Values:\", query_vector[0, :5].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(qryTower, docTower, dataloader, device):\n",
    "    qryTower.eval()\n",
    "    docTower.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get embeddings from batch\n",
    "        query_emb = batch['query']\n",
    "        pos_emb = batch['positive']\n",
    "        neg_emb = batch['negative']\n",
    "        \n",
    "        # Forward pass through towers\n",
    "        query_encoded = qryTower(query_emb)\n",
    "        pos_encoded = docTower(pos_emb)\n",
    "        neg_encoded = docTower(neg_emb)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_sim = torch.nn.functional.cosine_similarity(query_encoded, pos_encoded)\n",
    "        neg_sim = torch.nn.functional.cosine_similarity(query_encoded, neg_encoded)\n",
    "\n",
    "        correct += (pos_sim > neg_sim).sum().item()\n",
    "        total += batch['query'].size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Eval Accuracy (query closer to pos than neg): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53907333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(dataloader):.4f}\")\n",
    "evaluate_model(final_qry_tower, final_doc_tower, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "pos_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "\n",
    "# Ensure tensors have at least two dimensions before applying mean\n",
    "if query_emb.dim() == 1:\n",
    "\tquery_emb = query_emb.unsqueeze(0)\n",
    "if pos_emb.dim() == 1:\n",
    "\tpos_emb = pos_emb.unsqueeze(0)\n",
    "\n",
    "query_emb = query_emb.mean(dim=0)\n",
    "pos_emb = pos_emb.mean(dim=0)\n",
    "\n",
    "print(torch.nn.functional.cosine_similarity(query_emb, pos_emb, dim=0))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

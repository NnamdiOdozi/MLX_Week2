{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b0ec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnnamdi-odozi\u001b[0m (\u001b[33mnnamdi-odozi-ave-actuaries\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from load_models_and_data import load_vocabulary, load_embeddings, text_to_embeddings, calc_cosine_sim, calculate_embeddings, create_packed_batch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#from TwoTowerNN import QryTower, DocTower, TripletEmbeddingDataset, run_hyperparameter_tuning\n",
    "from TwinTowerGRU import QryTower, DocTower, EmbeddingTripletDataset, run_hyperparameter_tuning, GRUTwinTowerModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader,  SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b860fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba466e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading datasets from Hugging Face\n",
    "ds_soft_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_soft_negatives\")\n",
    "#ds_hard_neg = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1948ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and vocabulary...\n",
      "Loaded embeddings with shape: torch.Size([400000, 100])\n",
      "Loaded vocabulary with 399998 tokens\n",
      "Embedded text shape: torch.Size([26, 100])\n",
      "Embedding array with custom formatting:\n",
      "[[ 0.2616  0.4472 -0.0968 ... -0.4503  0.4952 -0.203 ]\n",
      " [ 0.1372 -0.5429  0.1942 ... -0.5206  0.2543 -0.2376]\n",
      " [-0.3046 -0.2365  0.1758 ... -0.8456 -0.0354  0.1704]\n",
      " ...\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]]\n",
      "Length is: 5\n"
     ]
    }
   ],
   "source": [
    "# Paths to your files\n",
    "embeddings_path = \"./downloaded_model/glove_embeddings.pt\" #set this to either own-trained cbow ones or to glove pre-trained ones\n",
    "vocab_path = \"./downloaded_model/glove_ids_to_words.csv\"\n",
    "\n",
    "# Load embeddings and vocabulary\n",
    "print(\"Loading embeddings and vocabulary...\")\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "word_to_idx = load_vocabulary(vocab_path)\n",
    "\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"Loaded vocabulary with {len(word_to_idx)} tokens\")\n",
    "\n",
    "# Example usage (uncomment when ready to test)\n",
    "sample_text = \"This is a test sentence\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "# Testing - Set numpy print options\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff426027",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\n",
    "embeddings_result, length = text_to_embeddings(sample_text, word_to_idx, embeddings, is_query=True)\n",
    "print(f\"Embedded text shape: {embeddings_result.shape}\")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=10)  # threshold limits number of elements shown\n",
    "numpy_array = embeddings_result.detach().numpy()\n",
    "print(\"Embedding array with custom formatting:\")\n",
    "print(numpy_array)\n",
    "print(\"Length is:\", length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a28244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg  = pd.DataFrame(ds_soft_neg['train'])\n",
    "#df_hard_neg  = pd.DataFrame(ds_hard_neg['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b81dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query, length = text_to_embeddings(df_soft_neg['query'][0], word_to_idx, embeddings, is_query=True)\n",
    "embedded_positive, length = text_to_embeddings(df_soft_neg['positive_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "embedded_negative, length = text_to_embeddings(df_soft_neg['negative_passage'][0], word_to_idx, embeddings, is_query=False)\n",
    "\n",
    "print(embedded_positive.shape)\n",
    "print(embedded_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedded_query.mean(dim=0)\n",
    "b = embedded_positive.mean(dim=0)\n",
    "c = embedded_negative.mean(dim=0)\n",
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cosine_similarity = F.cosine_similarity(a, c, dim=0)\n",
    "print(f\"Cosine similarity between query and positive passage: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Process the dataframe using apply just for first five rows\n",
    "# print(\"Calculating similarities... This may take a while depending on dataframe size.\")\n",
    "# similarities = df_soft_neg[0:5].progress_apply(\n",
    "#     lambda row: calculate_similarities(row, word_to_idx, embeddings), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Join the similarities to the dataframe\n",
    "# df_soft_neg_ext = pd.concat([df_soft_neg[0:5], similarities], axis=1)\n",
    "\n",
    "# # Show a sample of the results\n",
    "# #print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "#print(df_soft_neg_ext.head())\n",
    "#print(df_soft_neg_ext.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_soft_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_soft_neg_ext = pd.concat([df_soft_neg, embeddings_padded], axis=1)\n",
    "print(df_soft_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_soft_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_soft_neg_ext['query_pos_sim'] > df_soft_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_soft_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg_ext[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe using apply\n",
    "print(\"Calculating embeddings... This may take a while depending on dataframe size.\")\n",
    "embeddings_padded = df_hard_neg.progress_apply(\n",
    "    lambda row: calculate_embeddings(row, word_to_idx, embeddings), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Join the similarities to the dataframe\n",
    "df_hard_neg_ext = pd.concat([df_hard_neg, embeddings_padded], axis=1)\n",
    "print(df_hard_neg_ext.head())\n",
    "# Show a sample of the results\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].head())\n",
    "\n",
    "#print(df_hard_neg_ext[['query_pos_sim', 'query_neg_sim', 'pos_neg_sim']].mean())\n",
    "\n",
    "# Calculate how often the positive passage is ranked higher than negative\n",
    "#higher_count = (df_hard_neg_ext['query_pos_sim'] > df_hard_neg_ext['query_neg_sim']).sum()\n",
    "#total = len(df_hard_neg_ext)\n",
    "#print(f\"\\nPositive passage ranked higher than negative: {higher_count} out of {total} ({higher_count/total:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cea888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_neg_ext = pd.concat([df_soft_neg_ext, df_hard_neg_ext])\n",
    "df_all_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to pickle format\n",
    "df_soft_neg_ext.to_pickle(\"downloaded_model/df_soft_neg_ext.pkl\")\n",
    "#df_hard_neg_ext.to_pickle(\"downloaded_model/df_hard_neg_ext.pkl\")\n",
    "#df_all_neg_ext.to_pickle(\"downloaded_model/df_all_neg_ext.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02765d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a DataFrame from pickle if the file exists\n",
    "def load_df_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return pd.read_pickle(file_path)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Load DataFrames\n",
    "df_soft_neg_ext = load_df_if_exists(\"downloaded_model/df_soft_neg_ext.pkl\")\n",
    "#df_hard_neg_ext = load_df_if_exists(\"downloaded_model/df_hard_neg_ext.pkl\")\n",
    "#df_all_neg_ext = load_df_if_exists(\"downloaded_model/df_all_neg_ext.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_neg_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create packed sequences for RNN processing\n",
    "#packed_queries, packed_positives, packed_negatives = create_packed_batch(df_all_neg_ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd683001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Feed packed sequences to your RNN models\n",
    "# query_outputs, query_hidden =your_query_rnn(packed_queries)\n",
    "# pos_outputs, pos_hidden = your_document_rnn(packed_positives)\n",
    "# neg_outputs, neg_hidden = your_document_rnn(packed_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e0660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_151105-fxxgj3kh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/fxxgj3kh' target=\"_blank\">hyperparameter-tuning-20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/fxxgj3kh' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/fxxgj3kh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits: Train=47822 | Validation=15941 | Test=15941\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hyperparameter-tuning-20250425-151105</strong> at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/fxxgj3kh' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/fxxgj3kh</a><br> View project at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_151105-fxxgj3kh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_151106-cvnpymhh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/cvnpymhh' target=\"_blank\">dim100_batch512_hidden100_layers1_20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/cvnpymhh' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/cvnpymhh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training with: output_dim=100, batch_size=512, gru_hidden_dim=100, num_layers=1, dropout=0.1, lr=0.0005\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 (Train): 100%|██████████| 94/94 [00:11<00:00,  8.54it/s]\n",
      "Epoch 1/10 (Val): 100%|██████████| 32/32 [00:05<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1745, Val Loss: 0.1494, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 94/94 [00:09<00:00,  9.58it/s]\n",
      "Epoch 2/10 (Val): 100%|██████████| 32/32 [00:05<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1357, Val Loss: 0.1259, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 94/94 [00:09<00:00,  9.56it/s]\n",
      "Epoch 3/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.1181, Val Loss: 0.1142, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 94/94 [00:11<00:00,  8.03it/s]\n",
      "Epoch 4/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.1040, Val Loss: 0.1029, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 94/94 [00:08<00:00, 10.83it/s]\n",
      "Epoch 5/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0907, Val Loss: 0.0944, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 94/94 [00:09<00:00, 10.13it/s]\n",
      "Epoch 6/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0800, Val Loss: 0.0883, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 94/94 [00:09<00:00,  9.92it/s]\n",
      "Epoch 7/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0713, Val Loss: 0.0856, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 94/94 [00:09<00:00,  9.96it/s]\n",
      "Epoch 8/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0649, Val Loss: 0.0836, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 94/94 [00:08<00:00, 10.48it/s]\n",
      "Epoch 9/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0589, Val Loss: 0.0805, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 94/94 [00:09<00:00, 10.32it/s]\n",
      "Epoch 10/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0517, Val Loss: 0.0808, LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0.05174</td></tr><tr><td>val_loss</td><td>0.08082</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dim100_batch512_hidden100_layers1_20250425-151105</strong> at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/cvnpymhh' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/cvnpymhh</a><br> View project at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_151106-cvnpymhh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_151333-ndhvt13e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/ndhvt13e' target=\"_blank\">dim100_batch512_hidden100_layers1_20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/ndhvt13e' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/ndhvt13e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training with: output_dim=100, batch_size=512, gru_hidden_dim=100, num_layers=1, dropout=0.1, lr=0.001\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 94/94 [00:09<00:00,  9.95it/s]\n",
      "Epoch 1/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1579, Val Loss: 0.1384, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 94/94 [00:09<00:00, 10.07it/s]\n",
      "Epoch 2/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1205, Val Loss: 0.1132, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 94/94 [00:11<00:00,  8.47it/s]\n",
      "Epoch 3/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.1003, Val Loss: 0.0987, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 94/94 [00:11<00:00,  8.41it/s]\n",
      "Epoch 4/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0865, Val Loss: 0.0958, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.56it/s]\n",
      "Epoch 5/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0755, Val Loss: 0.0841, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.61it/s]\n",
      "Epoch 6/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0656, Val Loss: 0.0802, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 94/94 [00:11<00:00,  8.47it/s]\n",
      "Epoch 7/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0562, Val Loss: 0.0779, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.69it/s]\n",
      "Epoch 8/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0493, Val Loss: 0.0762, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.90it/s]\n",
      "Epoch 9/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0430, Val Loss: 0.0762, LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.83it/s]\n",
      "Epoch 10/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0372, Val Loss: 0.0754, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_loss</td><td>0.03718</td></tr><tr><td>val_loss</td><td>0.07535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dim100_batch512_hidden100_layers1_20250425-151105</strong> at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/ndhvt13e' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/ndhvt13e</a><br> View project at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_151333-ndhvt13e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_151608-uruk0o6r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/uruk0o6r' target=\"_blank\">dim100_batch512_hidden200_layers1_20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/uruk0o6r' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/uruk0o6r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training with: output_dim=100, batch_size=512, gru_hidden_dim=200, num_layers=1, dropout=0.1, lr=0.0005\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.94it/s]\n",
      "Epoch 1/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1650, Val Loss: 0.1371, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.97it/s]\n",
      "Epoch 2/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1289, Val Loss: 0.1226, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.84it/s]\n",
      "Epoch 3/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.1093, Val Loss: 0.1035, LR: 0.000500\n",
      "New best model saved with validation loss: 0.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.72it/s]\n",
      "Epoch 4/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0943, Val Loss: 0.0963, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.97it/s]\n",
      "Epoch 5/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0832, Val Loss: 0.0894, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.95it/s]\n",
      "Epoch 6/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0740, Val Loss: 0.0869, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.58it/s]\n",
      "Epoch 7/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0651, Val Loss: 0.0834, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.93it/s]\n",
      "Epoch 8/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0574, Val Loss: 0.0811, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.97it/s]\n",
      "Epoch 9/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0489, Val Loss: 0.0792, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.86it/s]\n",
      "Epoch 10/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0429, Val Loss: 0.0759, LR: 0.000500\n",
      "New best model saved with validation loss: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0.0429</td></tr><tr><td>val_loss</td><td>0.07592</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dim100_batch512_hidden200_layers1_20250425-151105</strong> at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/uruk0o6r' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/uruk0o6r</a><br> View project at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_151608-uruk0o6r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_151837-2605dhzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/2605dhzl' target=\"_blank\">dim100_batch512_hidden200_layers1_20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/2605dhzl' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/2605dhzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training with: output_dim=100, batch_size=512, gru_hidden_dim=200, num_layers=1, dropout=0.1, lr=0.001\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.91it/s]\n",
      "Epoch 1/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1542, Val Loss: 0.1321, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.84it/s]\n",
      "Epoch 2/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1194, Val Loss: 0.1128, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  9.07it/s]\n",
      "Epoch 3/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0999, Val Loss: 0.1048, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.61it/s]\n",
      "Epoch 4/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0848, Val Loss: 0.0905, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.92it/s]\n",
      "Epoch 5/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0736, Val Loss: 0.0873, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.83it/s]\n",
      "Epoch 6/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0642, Val Loss: 0.0802, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.73it/s]\n",
      "Epoch 7/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0570, Val Loss: 0.0854, LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  9.04it/s]\n",
      "Epoch 8/10 (Val): 100%|██████████| 32/32 [00:03<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0508, Val Loss: 0.0794, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.95it/s]\n",
      "Epoch 9/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0438, Val Loss: 0.0781, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 94/94 [00:10<00:00,  8.96it/s]\n",
      "Epoch 10/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0371, Val Loss: 0.0808, LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_loss</td><td>0.0371</td></tr><tr><td>val_loss</td><td>0.0808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dim100_batch512_hidden200_layers1_20250425-151105</strong> at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/2605dhzl' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/2605dhzl</a><br> View project at: <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_151837-2605dhzl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best hyperparameters:\n",
      "Output dimension: 100\n",
      "Batch size: 512\n",
      "GRU hidden dimension: 100\n",
      "Number of GRU layers: 1\n",
      "Dropout: 0.1\n",
      "Learning rate: 0.001\n",
      "Validation Loss: 0.0754\n",
      "\n",
      "\n",
      "Training final model with best hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.42it/s]\n",
      "Epoch 1/10 (Val): 100%|██████████| 32/32 [00:05<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1585, Val Loss: 0.1308, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.75it/s]\n",
      "Epoch 2/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.1218, Val Loss: 0.1138, LR: 0.001000\n",
      "New best model saved with validation loss: 0.1138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 125/125 [00:11<00:00, 11.33it/s]\n",
      "Epoch 3/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.1045, Val Loss: 0.0998, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.93it/s]\n",
      "Epoch 4/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0898, Val Loss: 0.0861, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.82it/s]\n",
      "Epoch 5/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0785, Val Loss: 0.0839, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.83it/s]\n",
      "Epoch 6/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0699, Val Loss: 0.0753, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.80it/s]\n",
      "Epoch 7/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0635, Val Loss: 0.0714, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.73it/s]\n",
      "Epoch 8/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0575, Val Loss: 0.0705, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 11.54it/s]\n",
      "Epoch 9/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0533, Val Loss: 0.0690, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 125/125 [00:10<00:00, 12.07it/s]\n",
      "Epoch 10/10 (Val): 100%|██████████| 32/32 [00:04<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0495, Val Loss: 0.0662, LR: 0.001000\n",
      "New best model saved with validation loss: 0.0662\n",
      "Final model saved at: checkpoints/final_gru_model_20250425-151105/final_gru_model_20250425-151105.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250425_152335-958z1i0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/958z1i0y' target=\"_blank\">final_model_20250425-151105</a></strong> to <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/958z1i0y' target=\"_blank\">https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/runs/958z1i0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W425 15:23:37.812899285 shape_type_inference.cpp:1995] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W425 15:23:37.817799832 shape_type_inference.cpp:1995] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "/root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "[W425 15:23:37.842232926 shape_type_inference.cpp:1995] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W425 15:23:37.844152130 shape_type_inference.cpp:1995] Warning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch IR graph at exception: graph(%embeddings.1 : Float(1, 26, 100, strides=[2600, 100, 1], requires_grad=0, device=cuda:0),\n",
      "      %1 : Long(1, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %embeddings : Float(1, 201, 100, strides=[20100, 100, 1], requires_grad=0, device=cuda:0),\n",
      "      %3 : Long(1, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %query_encoder.gru.weight_ih_l0 : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.weight_hh_l0 : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.bias_ih_l0 : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.bias_hh_l0 : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.weight_ih_l0_reverse : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.weight_hh_l0_reverse : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.bias_ih_l0_reverse : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_encoder.gru.bias_hh_l0_reverse : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.weight_ih_l0 : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.weight_hh_l0 : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.bias_ih_l0 : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.bias_hh_l0 : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.weight_ih_l0_reverse : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.weight_hh_l0_reverse : Float(300, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.bias_ih_l0_reverse : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_encoder.gru.bias_hh_l0_reverse : Float(300, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc1.weight : Float(128, 200, strides=[200, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc2.weight : Float(64, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc3.weight : Float(100, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %query_tower.fc3.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc1.weight : Float(128, 200, strides=[200, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc2.weight : Float(64, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc3.weight : Float(100, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %doc_tower.fc3.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %994 : Long(device=cpu) = prim::Constant[value={4}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %995 : Long(device=cpu) = prim::Constant[value={0}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %707 : Device = prim::Constant[value=\"cpu\"](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:74:0\n",
      "  %708 : NoneType = prim::Constant(), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %996 : Bool(device=cpu) = prim::Constant[value={0}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %712 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::to(%1, %994, %995, %707, %708, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:74:0\n",
      "  %717 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::to(%712, %994, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:331:0\n",
      "  %997 : Long(device=cpu) = prim::Constant[value={-1}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %998 : Bool(device=cpu) = prim::Constant[value={1}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %720 : Long(1, strides=[1], requires_grad=0, device=cpu), %721 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::sort(%717, %997, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:336:0\n",
      "  %724 : Device = prim::Constant[value=\"cuda:0\"](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:337:0\n",
      "  %sorted_indices.1 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%721, %994, %995, %724, %708, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:337:0\n",
      "  %731 : Float(1, 26, 100, strides=[2600, 100, 1], requires_grad=0, device=cuda:0) = aten::index_select(%embeddings.1, %995, %sorted_indices.1), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:339:0\n",
      "  %input.1 : Float(10, 100, strides=[100, 1], requires_grad=0, device=cuda:0), %batch_sizes.1 : Long(10, strides=[1], requires_grad=0, device=cpu) = aten::_pack_padded_sequence(%731, %720, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:341:0\n",
      "  %741 : Long(device=cpu) = aten::numel(%sorted_indices.1), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:275:0\n",
      "  %749 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::arange(%995, %741, %708, %708, %724, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:275:0\n",
      "  %740 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::empty_like(%sorted_indices.1, %708, %708, %708, %996, %995), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:273:0\n",
      "  %992 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::scatter(%740, %995, %sorted_indices.1, %749), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:274:0\n",
      "  %754 : Long(requires_grad=0, device=cpu) = aten::select(%batch_sizes.1, %995, %995), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1339:0\n",
      "  %999 : Long(device=cpu) = prim::Constant[value={2}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %1000 : Long(device=cpu) = prim::Constant[value={100}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %758 : int[] = prim::ListConstruct(%999, %754, %1000), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %1001 : Long(device=cpu) = prim::Constant[value={6}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %hidden.1 : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=0, device=cuda:0) = aten::zeros(%758, %1001, %708, %724, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1342:0\n",
      "  %787 : Tensor[] = prim::ListConstruct(%query_encoder.gru.weight_ih_l0, %query_encoder.gru.weight_hh_l0, %query_encoder.gru.bias_ih_l0, %query_encoder.gru.bias_hh_l0, %query_encoder.gru.weight_ih_l0_reverse, %query_encoder.gru.weight_hh_l0_reverse, %query_encoder.gru.bias_ih_l0_reverse, %query_encoder.gru.bias_hh_l0_reverse), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %1002 : Long(device=cpu) = prim::Constant[value={1}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %1003 : Double(device=cpu) = prim::Constant[value={0}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %793 : Float(10, 200, strides=[200, 1], requires_grad=1, device=cuda:0), %hx.1 : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::gru(%input.1, %batch_sizes.1, %hidden.1, %787, %998, %1002, %1003, %996, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1405:0\n",
      "  %796 : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::index_select(%hx.1, %1002, %992), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:37:0\n",
      "  %799 : Float(1, 2, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::transpose(%796, %995, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:83:0\n",
      "  %1004 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %804 : Float(1, 2, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::slice(%799, %995, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %1005 : Long(device=cpu) = prim::Constant[value={-2}](), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %807 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::select(%804, %1002, %1005), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %812 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::slice(%807, %1002, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %820 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::select(%804, %1002, %997), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:87:0\n",
      "  %825 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::slice(%820, %1002, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:87:0\n",
      "  %826 : Tensor[] = prim::ListConstruct(%812, %825), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder\n",
      "  %input.5 : Float(1, 200, strides=[200, 1], requires_grad=1, device=cuda:0) = aten::cat(%826, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::query_encoder # /root/MLX_Week2/TwinTowerGRU.py:88:0\n",
      "  %836 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::to(%3, %994, %995, %707, %708, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:74:0\n",
      "  %841 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::to(%836, %994, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:331:0\n",
      "  %844 : Long(1, strides=[1], requires_grad=0, device=cpu), %845 : Long(1, strides=[1], requires_grad=0, device=cpu) = aten::sort(%841, %997, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:336:0\n",
      "  %sorted_indices : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%845, %994, %995, %724, %708, %996, %996, %708), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:337:0\n",
      "  %855 : Float(1, 201, 100, strides=[20100, 100, 1], requires_grad=0, device=cuda:0) = aten::index_select(%embeddings, %995, %sorted_indices), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:339:0\n",
      "  %input.3 : Float(50, 100, strides=[100, 1], requires_grad=0, device=cuda:0), %batch_sizes : Long(50, strides=[1], requires_grad=0, device=cpu) = aten::_pack_padded_sequence(%855, %844, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:341:0\n",
      "  %865 : Long(device=cpu) = aten::numel(%sorted_indices), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:275:0\n",
      "  %873 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::arange(%995, %865, %708, %708, %724, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:275:0\n",
      "  %864 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::empty_like(%sorted_indices, %708, %708, %708, %996, %995), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:273:0\n",
      "  %993 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = aten::scatter(%864, %995, %sorted_indices, %873), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/utils/rnn.py:274:0\n",
      "  %878 : Long(requires_grad=0, device=cpu) = aten::select(%batch_sizes, %995, %995), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1339:0\n",
      "  %882 : int[] = prim::ListConstruct(%999, %878, %1000), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %hidden : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=0, device=cuda:0) = aten::zeros(%882, %1001, %708, %724, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1342:0\n",
      "  %911 : Tensor[] = prim::ListConstruct(%doc_encoder.gru.weight_ih_l0, %doc_encoder.gru.weight_hh_l0, %doc_encoder.gru.bias_ih_l0, %doc_encoder.gru.bias_hh_l0, %doc_encoder.gru.weight_ih_l0_reverse, %doc_encoder.gru.weight_hh_l0_reverse, %doc_encoder.gru.bias_ih_l0_reverse, %doc_encoder.gru.bias_hh_l0_reverse), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru\n",
      "  %917 : Float(50, 200, strides=[200, 1], requires_grad=1, device=cuda:0), %hx : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::gru(%input.3, %batch_sizes, %hidden, %911, %998, %1002, %1003, %996, %998), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1405:0\n",
      "  %920 : Float(2, 1, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::index_select(%hx, %1002, %993), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder/torch.nn.modules.rnn.GRU::gru # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:37:0\n",
      "  %923 : Float(1, 2, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::transpose(%920, %995, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:83:0\n",
      "  %928 : Float(1, 2, 100, strides=[100, 100, 1], requires_grad=1, device=cuda:0) = aten::slice(%923, %995, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %931 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::select(%928, %1002, %1005), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %936 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::slice(%931, %1002, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:86:0\n",
      "  %944 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::select(%928, %1002, %997), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:87:0\n",
      "  %949 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::slice(%944, %1002, %995, %1004, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:87:0\n",
      "  %950 : Tensor[] = prim::ListConstruct(%936, %949), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder\n",
      "  %input.11 : Float(1, 200, strides=[200, 1], requires_grad=1, device=cuda:0) = aten::cat(%950, %1002), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.BidirectionalGRU::doc_encoder # /root/MLX_Week2/TwinTowerGRU.py:88:0\n",
      "  %1006 : Double(device=cpu) = prim::Constant[value={0.1}](), scope: TwinTowerGRU.GRUTwinTowerModel::/torch.nn.modules.dropout.Dropout::dropout\n",
      "  %955 : Float(1, 200, strides=[200, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.5, %1006, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/torch.nn.modules.dropout.Dropout::dropout # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.7 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::linear(%955, %query_tower.fc1.weight, %query_tower.fc1.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.QryTower::query_tower/torch.nn.modules.linear.Linear::fc1 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %957 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.7), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.QryTower::query_tower/torch.nn.modules.activation.ReLU::relu # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1704:0\n",
      "  %input.9 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = aten::linear(%957, %query_tower.fc2.weight, %query_tower.fc2.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.QryTower::query_tower/torch.nn.modules.linear.Linear::fc2 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %959 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.9), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.QryTower::query_tower/torch.nn.modules.activation.ReLU::relu # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1704:0\n",
      "  %query_vector : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::linear(%959, %query_tower.fc3.weight, %query_tower.fc3.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.QryTower::query_tower/torch.nn.modules.linear.Linear::fc3 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %963 : Float(1, 200, strides=[200, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input.11, %1006, %996), scope: TwinTowerGRU.GRUTwinTowerModel::/torch.nn.modules.dropout.Dropout::dropout # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.13 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::linear(%963, %doc_tower.fc1.weight, %doc_tower.fc1.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.DocTower::doc_tower/torch.nn.modules.linear.Linear::fc1 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %965 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.13), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.DocTower::doc_tower/torch.nn.modules.activation.ReLU::relu # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1704:0\n",
      "  %input : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = aten::linear(%965, %doc_tower.fc2.weight, %doc_tower.fc2.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.DocTower::doc_tower/torch.nn.modules.linear.Linear::fc2 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %967 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = aten::relu(%input), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.DocTower::doc_tower/torch.nn.modules.activation.ReLU::relu # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:1704:0\n",
      "  %doc_vector : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::linear(%967, %doc_tower.fc3.weight, %doc_tower.fc3.bias), scope: TwinTowerGRU.GRUTwinTowerModel::/TwinTowerGRU.DocTower::doc_tower/torch.nn.modules.linear.Linear::fc3 # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %990 : int[] = prim::Constant[value=[1]]()\n",
      "  %974 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cuda:0) = aten::linalg_vector_norm(%query_vector, %999, %990, %998, %708), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/functional.py:1827:0\n",
      "  %1007 : Double(device=cpu) = prim::Constant[value={1e-12}](), scope: TwinTowerGRU.GRUTwinTowerModel::\n",
      "  %976 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cuda:0) = aten::clamp_min(%974, %1007), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5486:0\n",
      "  %977 : Float(1, 100, strides=[1, 0], requires_grad=1, device=cuda:0) = aten::expand_as(%976, %query_vector), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5486:0\n",
      "  %978 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::div(%query_vector, %977), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5487:0\n",
      "  %984 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cuda:0) = aten::linalg_vector_norm(%doc_vector, %999, %990, %998, %708), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/functional.py:1827:0\n",
      "  %986 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cuda:0) = aten::clamp_min(%984, %1007), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5486:0\n",
      "  %987 : Float(1, 100, strides=[1, 0], requires_grad=1, device=cuda:0) = aten::expand_as(%986, %doc_vector), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5486:0\n",
      "  %988 : Float(1, 100, strides=[100, 1], requires_grad=1, device=cuda:0) = aten::div(%doc_vector, %987), scope: TwinTowerGRU.GRUTwinTowerModel:: # /root/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/nn/functional.py:5487:0\n",
      "  return (%978, %988)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vector::_M_range_check: __n (which is 0) >= this->size() (which is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear CUDA cache if using GPU\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_hyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_soft_neg_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru_hidden_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MLX_Week2/TwinTowerGRU.py:690\u001b[0m, in \u001b[0;36mrun_hyperparameter_tuning\u001b[0;34m(df, output_dims, batch_sizes, gru_hidden_dims, num_layers, dropouts, learning_rates, epochs, log_wandb)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# Export directly to a temporary file\u001b[39;00m\n\u001b[1;32m    689\u001b[0m onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_checkpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 690\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_query_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_doc_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43monnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_vector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument_vector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Add to artifact\u001b[39;00m\n\u001b[1;32m    695\u001b[0m final_model_artifact\u001b[38;5;241m.\u001b[39madd_file(onnx_path)\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/__init__.py:383\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe exporter only supports dynamic shapes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m     )\n\u001b[0;32m--> 383\u001b[0m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/utils.py:495\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;241m+\u001b[39m (kwargs,)\n\u001b[0;32m--> 495\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/utils.py:1428\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1425\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1426\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1428\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_opsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     custom_opsets \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/utils.py:1057\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1054\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1057\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_onnx_log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/MLX_Week2/mlx_env/lib/python3.10/site-packages/torch/onnx/utils.py:641\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    636\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_scalar_type_analysis(\n\u001b[1;32m    637\u001b[0m     graph, \u001b[38;5;28;01mTrue\u001b[39;00m, GLOBALS\u001b[38;5;241m.\u001b[39mexport_onnx_opset_version\n\u001b[1;32m    638\u001b[0m )\n\u001b[1;32m    639\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n\u001b[0;32m--> 641\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx_peephole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLOBALS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx_opset_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# graph is not a valid jit graph anymore because types have been replaced\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# (e.g. int with Tensor), so it now contains operators that don't actually\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# exist. We can't run normal dead code elimination because it'd fail trying\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# to look up if an operator has side effects, but we can run a dead code\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# elimination variant that doesn't need to look up if an op has side effects.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vector::_M_range_check: __n (which is 0) >= this->size() (which is 0)"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # Clear CUDA cache if using GPU\n",
    "\n",
    "\n",
    "run_hyperparameter_tuning(df=df_soft_neg_ext, output_dims=[100], batch_sizes=[512], gru_hidden_dims=[100, 200], \n",
    "                         num_layers=[1], dropouts=[0.1], learning_rates=[0.0005, 1e-3], \n",
    "                         epochs=10, log_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3edcd",
   "metadata": {},
   "source": [
    "### Twin Tower Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05391efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"checkpoints\", \"final_gru_model_20250424-152045\", \"final_gru_model_20250424-152045.pt\")\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model instance\n",
    "model = GRUTwinTowerModel(embedding_dim=100, gru_hidden_dim=100, output_dim=100, \n",
    "                         num_layers=1, dropout=0.1)\n",
    "\n",
    "# Load the checkpoint and extract the model state dict\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "# The error shows the state_dict is nested under \"model_state_dict\"\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model from WandB\n",
    "#run = wandb.init()\n",
    "# The correct artifact path format\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(\"nnamdi-odozi-ave-actuaries/gru-twin-tower-model/final_gru_model_20250424-174424:v0\")\n",
    "\n",
    "#https://wandb.ai/nnamdi-odozi-ave-actuaries/gru-twin-tower-model/artifacts/model/final_gru_model_20250424-174424/v0/files/final_gru_model_20250424-174424.pt\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Find the model file\n",
    "import os\n",
    "model_files = [f for f in os.listdir(artifact_dir) if f.endswith('.pt') or f.endswith('.pth')]\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"No model files found in {artifact_dir}\")\n",
    "\n",
    "model_path = os.path.join(artifact_dir, model_files[0])\n",
    "print(f\"Found model at: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Create model with correct dimensions\n",
    "model2 = GRUTwinTowerModel(\n",
    "    embedding_dim=100, \n",
    "    gru_hidden_dim=100,  # Use 100 as seen in your model print\n",
    "    output_dim=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Load state dict (handle both formats)\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    model2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    model2.load_state_dict(checkpoint)\n",
    "\n",
    "model2 = model.to(device)\n",
    "model2.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test with consecutive rows - just use a slice directly\n",
    "df_slice = df_soft_neg_ext[0:3]  # Use any 3 consecutive rows\n",
    "\n",
    "# Process dataframe slice\n",
    "with torch.no_grad():\n",
    "    # Move everything to device\n",
    "    query_embs = torch.stack(df_slice['query_emb'].tolist()).to(device)\n",
    "    query_lens = torch.tensor(df_slice['query_length'].tolist()).to(device)\n",
    "    pos_embs = torch.stack(df_slice['pos_emb'].tolist()).to(device)\n",
    "    pos_lens = torch.tensor(df_slice['pos_length'].tolist()).to(device)\n",
    "    \n",
    "    # Get encodings for all rows at once\n",
    "    query_vecs, doc_vecs = model(query_embs, query_lens, pos_embs, pos_lens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = torch.nn.functional.cosine_similarity(query_vecs, doc_vecs, dim=1)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b10fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test with consecutive rows - just use a slice directly\n",
    "df_slice = df_soft_neg_ext[0:512]  # Doing more rows\n",
    "\n",
    "# Process dataframe slice\n",
    "with torch.no_grad():\n",
    "    # Move everything to device\n",
    "    query_embs = torch.stack(df_slice['query_emb'].tolist()).to(device)\n",
    "    query_lens = torch.tensor(df_slice['query_length'].tolist()).to(device)\n",
    "    pos_embs = torch.stack(df_slice['pos_emb'].tolist()).to(device)\n",
    "    pos_lens = torch.tensor(df_slice['pos_length'].tolist()).to(device)\n",
    "    \n",
    "    # Get encodings for all rows at once\n",
    "    query_vecs, doc_vecs = model(query_embs, query_lens, pos_embs, pos_lens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sims = torch.nn.functional.cosine_similarity(query_vecs, doc_vecs, dim=1)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())\n",
    "sims.cpu().numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd960918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with a random sentence:\n",
    "query_test = \"This is RBA\"\n",
    "doc_test = \"This is RBA\"\n",
    "q_l = len(query_test.split())\n",
    "d_l = len(doc_test.split())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f984ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb, q_l = text_to_embeddings(query_test, word_to_idx, embeddings, is_query=True)\n",
    "doc_emb, d_l = text_to_embeddings(doc_test, word_to_idx, embeddings, is_query=False)\n",
    "print(query_emb.shape)\n",
    "print(doc_emb)\n",
    "print(q_l, d_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64612c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_emb.mean(dim=0)\n",
    "d = doc_emb.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43464337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "sims = torch.nn.functional.cosine_similarity(q, d, dim=0)\n",
    "    \n",
    "print(\"Similarities:\", sims.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab178558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make sure tensors are on the right device\n",
    "device = next(model.parameters()).device\n",
    "query_emb = query_emb.to(device)  # Shape should be [seq_length, embedding_dim]\n",
    "q_l = torch.tensor([q_l], device=device)  # Single value for sequence length\n",
    "\n",
    "# 2. Add batch dimension for model processing\n",
    "query_emb = query_emb.unsqueeze(0)  # Shape becomes [1, seq_length, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Query-only inference using just dataframe columns \n",
    "#query_row = df_soft_neg_ext[0]  # Use any row\n",
    "#test_query_emb = query_row['query_emb'].unsqueeze(0).to(device)\n",
    "#test_query_len = torch.tensor([query_row['query_length']]).to(device)\n",
    "\n",
    "# Just run through query encoder and tower\n",
    "with torch.no_grad():\n",
    "    query_encoded = model.query_encoder(query_emb, q_l)\n",
    "    query_vector = model.query_tower(query_encoded)\n",
    "    query_vector = torch.nn.functional.normalize(query_vector, p=2, dim=1) #I don't think this is needed, but let's keep it for now\n",
    "\n",
    "print(\"Query vector shape:\", query_vector.shape)\n",
    "print(\"Values:\", query_vector[0, :5].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(qryTower, docTower, dataloader, device):\n",
    "    qryTower.eval()\n",
    "    docTower.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get embeddings from batch\n",
    "        query_emb = batch['query']\n",
    "        pos_emb = batch['positive']\n",
    "        neg_emb = batch['negative']\n",
    "        \n",
    "        # Forward pass through towers\n",
    "        query_encoded = qryTower(query_emb)\n",
    "        pos_encoded = docTower(pos_emb)\n",
    "        neg_encoded = docTower(neg_emb)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_sim = torch.nn.functional.cosine_similarity(query_encoded, pos_encoded)\n",
    "        neg_sim = torch.nn.functional.cosine_similarity(query_encoded, neg_encoded)\n",
    "\n",
    "        correct += (pos_sim > neg_sim).sum().item()\n",
    "        total += batch['query'].size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Eval Accuracy (query closer to pos than neg): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53907333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(dataloader):.4f}\")\n",
    "evaluate_model(final_qry_tower, final_doc_tower, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "pos_emb = text_to_embeddings(\"What is RBA\", word_to_idx, embeddings)\n",
    "\n",
    "# Ensure tensors have at least two dimensions before applying mean\n",
    "if query_emb.dim() == 1:\n",
    "\tquery_emb = query_emb.unsqueeze(0)\n",
    "if pos_emb.dim() == 1:\n",
    "\tpos_emb = pos_emb.unsqueeze(0)\n",
    "\n",
    "query_emb = query_emb.mean(dim=0)\n",
    "pos_emb = pos_emb.mean(dim=0)\n",
    "\n",
    "print(torch.nn.functional.cosine_similarity(query_emb, pos_emb, dim=0))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings\n",
    "We are retraining embeddings based on the cbow architecture in word2vec. \n",
    "\n",
    "First, tokenize the combined text8 and ms-marco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the text before filtering:  74281642\n",
      "Number of words in the text after filtering:  73326867\n",
      "Number of words in the vocabulary:  146489\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import preprocess, create_lookup_tables\n",
    "import pickle\n",
    "\n",
    "# open the combined_text8_msmarco.txt file and read the text\n",
    "with open('./data/temp/combined_text8_msmarco.txt', 'r') as f:\n",
    "    combined_text = f.read()\n",
    "\n",
    "# tokenize the text and save as a .pkl file\n",
    "combined_tokens = preprocess(combined_text)\n",
    "with open('./data/temp/combined_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_tokens, f)\n",
    "\n",
    "# create the lookup tables\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(combined_tokens)\n",
    "\n",
    "# save the lookup tables to a .pkl file\n",
    "with open('./data/temp/combined_vocab_to_int.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_to_int, f)\n",
    "\n",
    "with open('./data/temp/combined_int_to_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(int_to_vocab, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our new vocabulary from the combined text8 wiki data and ms-marco data, let's generate the embeddings using the CBOW architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(corpus[:100])\n",
    "\n",
    "# Generate the training data from the corpus\n",
    "# The training data looks like a list of tuples,\n",
    "# where each tuple contains a list of context words and the target word (not the IDs)\n",
    "\n",
    "\n",
    "def generate_training_data(corpus):\n",
    "   data = []\n",
    "\n",
    "   # start from index 2 and end 2 positions before the last word\n",
    "   # this ensures we always have 2 words before and after the target word\n",
    "   # for a 5-len sliding window\n",
    "\n",
    "\n",
    "   for i in range(2, len(corpus) - 2):\n",
    "       # Get the context words\n",
    "       # 'i' is the index of the target word\n",
    "       # [i-2:i] gets the two words before the target word\n",
    "       # [i+1:i+3] gets the two words after the target word\n",
    "       context_words = corpus[i-2:i] + corpus[i+1:i+3]\n",
    "      \n",
    "       # Get the target word\n",
    "       target_word = corpus[i]\n",
    "\n",
    "\n",
    "       # Append the tuple to the data list\n",
    "       data.append((context_words, target_word))\n",
    "\n",
    "\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "\n",
    "# load the corpus\n",
    "with open('./data/temp/combined_corpus.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "training_data = generate_training_data(corpus)\n",
    "print(\"CBOW training data generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['esp', 'magnetic', 'or', 'electric'], 'poles'),\n",
       " (['magnetic', 'poles', 'electric', 'charge'], 'or'),\n",
       " (['poles', 'or', 'charge', '<PERIOD>'], 'electric'),\n",
       " (['or', 'electric', '<PERIOD>', '3'], 'charge'),\n",
       " (['electric', 'charge', '3', '<PERIOD>'], '<PERIOD>'),\n",
       " (['charge', '<PERIOD>', '<PERIOD>', '<LEFT_PAREN>'], '3'),\n",
       " (['<PERIOD>', '3', '<LEFT_PAREN>', 'general'], '<PERIOD>'),\n",
       " (['3', '<PERIOD>', 'general', 'physics'], '<LEFT_PAREN>'),\n",
       " (['<PERIOD>', '<LEFT_PAREN>', 'physics', '<RIGHT_PAREN>'], 'general'),\n",
       " (['<LEFT_PAREN>', 'general', '<RIGHT_PAREN>', 'the'], 'physics'),\n",
       " (['general', 'physics', 'the', 'particular'], '<RIGHT_PAREN>'),\n",
       " (['physics', '<RIGHT_PAREN>', 'particular', 'state'], 'the'),\n",
       " (['<RIGHT_PAREN>', 'the', 'state', 'of'], 'particular'),\n",
       " (['the', 'particular', 'of', 'a'], 'state'),\n",
       " (['particular', 'state', 'a', 'part'], 'of'),\n",
       " (['state', 'of', 'part', 'of'], 'a'),\n",
       " (['of', 'a', 'of', 'a'], 'part'),\n",
       " (['a', 'part', 'a', 'body'], 'of'),\n",
       " (['part', 'of', 'body', 'or'], 'a'),\n",
       " (['of', 'a', 'or', 'system'], 'body'),\n",
       " (['a', 'body', 'system', 'that'], 'or'),\n",
       " (['body', 'or', 'that', 'has'], 'system'),\n",
       " (['or', 'system', 'has', 'polarity'], 'that'),\n",
       " (['system', 'that', 'polarity', '<COLON>'], 'has'),\n",
       " (['that', 'has', '<COLON>', 'an'], 'polarity'),\n",
       " (['has', 'polarity', 'an', 'electrode'], '<COLON>'),\n",
       " (['polarity', '<COLON>', 'electrode', 'with'], 'an'),\n",
       " (['<COLON>', 'an', 'with', 'positive'], 'electrode'),\n",
       " (['an', 'electrode', 'positive', 'polarity'], 'with'),\n",
       " (['electrode', 'with', 'polarity', '<PERIOD>'], 'positive')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick eyeball check to see if the training data is correct\n",
    "# show the last 30 tuples in the training data\n",
    "training_data[-30:]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"\n",
    "UNCOMMENT THIS SECTION TO PUSH THE DATA TO HUGGING FACE\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Push the combined corpus, int_to_vocab, and vocab_to_int to Hugging Face\n",
    "# Make sure have a HF account and token, save token to a .env file\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "pt_files = glob.glob(\"./data/temp/*\")\n",
    "for file in pt_files:\n",
    "    print(\"Currently uploading\", file)\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=os.path.basename(file),\n",
    "        repo_id=\"titaneve/text8-and-msmarco\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    print(f\"Successfully uploaded {file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we start training the cbow model (word2vec) to get the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import tqdm\n",
    "import wandb\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the data input structure and the model\n",
    "class CBOWDataset(Dataset):\n",
    "   def __init__(self, data: list[tuple[list[str], str]], word_to_id: dict[str, int]):\n",
    "       self.data = data\n",
    "       self.word_to_id = word_to_id\n",
    "\n",
    "\n",
    "   # overriding the __len__ method to tell PyTorch how many samples you have\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "\n",
    "\n",
    "   # overriding the __getitem__ method\n",
    "   # to tell PyTorch how to retrieve a specific sample and convert it to the format your model expects\n",
    "   def __getitem__(self, idx):\n",
    "       context, target = self.data[idx]\n",
    "       context_ids = torch.tensor([self.word_to_id[word] for word in context], dtype=torch.long)\n",
    "       target_id = torch.tensor(self.word_to_id[target], dtype=torch.long)\n",
    "       return context_ids, target_id\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "   def __init__(self, vocab_size, embedding_dim):\n",
    "       super(CBOW, self).__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, inputs):\n",
    "       embed = self.embedding(inputs)\n",
    "       embed = embed.mean(dim=1)\n",
    "       out = self.linear(embed)\n",
    "       probs = F.log_softmax(out, dim=1)\n",
    "       return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4000\n",
      "GPU Memory: 15.82 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize settings\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Print that the GPU is being used\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"\n",
    " UNCOMMENT THIS SECTION IF LOADING FROM LOCAL \n",
    "\"\"\"\n",
    "\n",
    "# Load the word_to_id and training_data from the temp folder\n",
    "with open('./data/temp/combined_vocab_to_int.pkl', 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "\n",
    "with open('./data/temp/combined_training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "word_to_id[:10]\n",
    "training_data[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146490\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>▆█▇▇▅▅▅▅▅▅▄▄▅▃▃▅▂▄▂▄▂▃▂▄▃▁▂▂▃▃▂▃▂▁▁▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>6.47462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025_04_22__12_52_34</strong> at: <a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings/runs/ioydyp8f' target=\"_blank\">https://wandb.ai/evelyntants-personal/word2vec_embeddings/runs/ioydyp8f</a><br> View project at: <a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings' target=\"_blank\">https://wandb.ai/evelyntants-personal/word2vec_embeddings</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250422_125241-ioydyp8f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MLX_Week2/wandb/run-20250422_125820-2aepid11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings/runs/2aepid11' target=\"_blank\">2025_04_22__12_58_14</a></strong> to <a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings' target=\"_blank\">https://wandb.ai/evelyntants-personal/word2vec_embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evelyntants-personal/word2vec_embeddings/runs/2aepid11' target=\"_blank\">https://wandb.ai/evelyntants-personal/word2vec_embeddings/runs/2aepid11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "\n",
    "dataset = CBOWDataset(training_data, vocab_to_int)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Initialize wandb with your configuration\n",
    "wandb.init(\n",
    "   entity=\"evelyntants-personal\",\n",
    "   project=\"word2vec_embeddings\",\n",
    "   name=f\"{timestamp}\",\n",
    "   config={\n",
    "       # Model parameters\n",
    "       \"embedding_dim\": 200,\n",
    "       \"vocab_size\": 146490,\n",
    "      \n",
    "       # Training parameters\n",
    "       \"batch_size\": 1024,\n",
    "       \"learning_rate\": 0.003,\n",
    "       \"num_epochs\": 5,\n",
    "       \"train_split\": 0.7,\n",
    "      \n",
    "       # Optimizer parameters\n",
    "       \"weight_decay\": 1e-5,\n",
    "      \n",
    "       # DataLoader parameters\n",
    "       \"num_workers\": 4,\n",
    "      \n",
    "       # Architecture details\n",
    "       \"model_type\": \"CBOW\",\n",
    "       \"context_size\": 4  # 2 words before + 2 words after\n",
    "   }\n",
    ")\n",
    "\n",
    "# Then use the config values throughout your code\n",
    "EMBEDDING_DIM = wandb.config.embedding_dim\n",
    "BATCH_SIZE = wandb.config.batch_size\n",
    "LEARNING_RATE = wandb.config.learning_rate\n",
    "NUM_EPOCHS = wandb.config.num_epochs\n",
    "TRAIN_SPLIT = wandb.config.train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=wandb.config.num_workers     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=wandb.config.num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(\n\u001b[1;32m      2\u001b[0m    vocab_size\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      3\u001b[0m    embedding_dim\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39membedding_dim)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     11\u001b[0m                             lr\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     12\u001b[0m                             weight_decay\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mweight_decay)\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(\n",
    "   vocab_size=wandb.config.vocab_size,\n",
    "   embedding_dim=wandb.config.embedding_dim)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=wandb.config.learning_rate,\n",
    "                            weight_decay=wandb.config.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "#Add evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "   model.eval()\n",
    "   total_loss = 0\n",
    "   with torch.no_grad():\n",
    "      for context, target in test_loader:\n",
    "           context, target = context.to(device), target.to(device)\n",
    "           output = model(context)\n",
    "           loss = criterion(output, target)\n",
    "           total_loss += loss.item()\n",
    "   return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in do_one_step\n    data = pin_memory(data, device)\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 98, in pin_memory\n    clone[i] = pin_memory(item, device)\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 64, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m progress:\n\u001b[1;32m      8\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/MLX_Week2/.venv/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in do_one_step\n    data = pin_memory(data, device)\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 98, in pin_memory\n    clone[i] = pin_memory(item, device)\n  File \"/root/MLX_Week2/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 64, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "# simplified training loop\n",
    "# Training Loop\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)\n",
    "    for inputs, targets in progress:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        wandb.log({'batch_loss': loss.item()})\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "    # Log epoch metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': avg_test_loss,\n",
    "    })\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{wandb.config.num_epochs}: Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # Save model checkpoint after every epoch\n",
    "    checkpoint_path = f\"../model/cbow_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    model_artifact = wandb.Artifact('model-weights', type='model')\n",
    "    model_artifact.add_file(checkpoint_path)\n",
    "    wandb.log_artifact(model_artifact)\n",
    "\n",
    "    # Save embeddings separately\n",
    "    embedding_weights = model.embedding.weight.data.cpu()\n",
    "    embedding_path = f\"../model/embeddings_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(embedding_weights, embedding_path)\n",
    "    embedding_artifact = wandb.Artifact('embeddings', type='embeddings')\n",
    "    embedding_artifact.add_file(embedding_path)\n",
    "    wandb.log_artifact(embedding_artifact)\n",
    "\n",
    "# Finish Wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

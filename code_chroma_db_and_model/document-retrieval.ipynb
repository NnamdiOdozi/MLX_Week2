{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e36abce",
   "metadata": {},
   "source": [
    "## Top-5 Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa79cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "  def __init__(self, voc, emb):\n",
    "    super().__init__()\n",
    "    self.embeddings = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.linear = torch.nn.Linear(in_features=emb, out_features=voc)\n",
    "\n",
    "  def forward(self, inpt):\n",
    "    emb = self.embeddings(inpt)\n",
    "    emb = emb.mean(dim=1)\n",
    "    out = self.linear(emb)\n",
    "    return out\n",
    "\n",
    "\n",
    "class QryTower(nn.Module):\n",
    "    def __init__(self, embedding_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # 1. Sort by descending length\n",
    "        lengths, sort_idx = lengths.sort(descending=True)\n",
    "        x = x[sort_idx]\n",
    "\n",
    "        # 2. Pack\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        # 3. GRU\n",
    "        _, hidden = self.rnn(packed)  # hidden: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # 4. Unsort\n",
    "        _, unsort_idx = sort_idx.sort()\n",
    "        hidden = hidden.squeeze(0)[unsort_idx]  # [batch_size, hidden_dim]\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embedding_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # 1. Sort by descending length\n",
    "        lengths, sort_idx = lengths.sort(descending=True)\n",
    "        x = x[sort_idx]\n",
    "\n",
    "        # 2. Pack\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        # 3. GRU\n",
    "        _, hidden = self.rnn(packed)  # hidden: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # 4. Unsort\n",
    "        _, unsort_idx = sort_idx.sort()\n",
    "        hidden = hidden.squeeze(0)[unsort_idx]  # [batch_size, hidden_dim]\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a5673",
   "metadata": {},
   "source": [
    "## retrieving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "045d7ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready for inference!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models with same architecture\n",
    "qryTower = QryTower().to(device)\n",
    "docTower = DocTower().to(device)\n",
    "\n",
    "# Load saved checkpoint\n",
    "checkpoint = torch.load(\"two_tower_model_GRU_padding.pt\", map_location=device)\n",
    "\n",
    "qryTower.load_state_dict(checkpoint[\"qryTower\"])\n",
    "docTower.load_state_dict(checkpoint[\"docTower\"])\n",
    "token_to_index = checkpoint[\"token_to_index\"]\n",
    "\n",
    "qryTower.eval()\n",
    "docTower.eval()\n",
    "\n",
    "print(\"✅ Model loaded and ready for inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846c59a",
   "metadata": {},
   "source": [
    "## Precompute Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, df, token_to_index, embedding_layer, device):\n",
    "        self.df = df\n",
    "        self.token_to_index = token_to_index\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_dim = embedding_layer.embedding_dim\n",
    "        self.oov_embeddings = {}  # For storing fixed random vectors for OOV tokens\n",
    "\n",
    "        self.query_max_len = max(len(text.lower().split()) for text in df[\"query\"])\n",
    "        all_docs = df[\"positive_passage\"].tolist() + df[\"negative_passage\"].tolist()\n",
    "        self.doc_max_len = max(len(text.lower().split()) for text in all_docs)\n",
    "\n",
    "    def embed(self, token):\n",
    "        \"\"\"Return embedding for token: from vocab or generate fixed OOV vector.\"\"\"\n",
    "        if token in self.token_to_index:\n",
    "            idx = self.token_to_index[token]\n",
    "            return self.embedding_layer(torch.tensor(idx, device=self.device))\n",
    "        else:\n",
    "            if token not in self.oov_embeddings:\n",
    "                self.oov_embeddings[token] = torch.randn(self.embedding_dim, device=self.device) * 0.1\n",
    "            return self.oov_embeddings[token]\n",
    "    \n",
    "    def embed_text(self, text, max_len):\n",
    "        tokens = text.lower().split()\n",
    "        embedded_tokens = []\n",
    "\n",
    "        for tok in tokens[:max_len]:\n",
    "            emb = self.embed(tok)\n",
    "            embedded_tokens.append(emb)\n",
    "\n",
    "        true_len = len(embedded_tokens)\n",
    "\n",
    "        # Padding with vector at index 0\n",
    "        pad_len = max_len - true_len\n",
    "        if pad_len > 0:\n",
    "            pad_vec = self.embedding_layer(torch.tensor(0, device=self.device))  # index 0 used for padding\n",
    "            embedded_tokens.extend([pad_vec] * pad_len)\n",
    "\n",
    "        embedded = torch.stack(embedded_tokens)\n",
    "        return embedded, true_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        query, q_len = self.embed_text(row[\"query\"], self.query_max_len)\n",
    "        pos, p_len   = self.embed_text(row[\"positive_passage\"], self.doc_max_len)\n",
    "        neg, n_len   = self.embed_text(row[\"negative_passage\"], self.doc_max_len)\n",
    "\n",
    "        return query, q_len, pos, p_len, neg, n_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef8a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56c0c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m docTower\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m all_doc_texts \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive_passage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;241m1000\u001b[39m]  \u001b[38;5;66;03m# Adjust this as needed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m doc_vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docTower.eval()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_sn = load_dataset(\"cocoritzy/week_2_triplet_dataset_soft_negatives\")\n",
    "# dataset = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\", split=\"train[:10%]\") # 10% of the datab\n",
    "\n",
    "df_sn = df_sn[\"train\"].to_pandas()\n",
    "\n",
    "all_doc_texts = df_sn[\"positive_passage\"].tolist()[:1000]  # Adjust this as needed\n",
    "\n",
    "doc_vectors = []\n",
    "with torch.no_grad():\n",
    "    for text in all_doc_texts:\n",
    "        doc_tensor, doc_len = triplet_dataset.embed_text(text, triplet_dataset.doc_max_len)\n",
    "        doc_tensor = doc_tensor.unsqueeze(0).to(device)\n",
    "        doc_len = torch.tensor([doc_len], device=device)\n",
    "\n",
    "        doc_vec = docTower(doc_tensor, doc_len)  # [1, hidden_dim]\n",
    "        doc_vectors.append(doc_vec.squeeze(0))\n",
    "\n",
    "doc_matrix = torch.stack(doc_vectors).to(device)  # [num_docs, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qryTower.eval()\n",
    "\n",
    "query = \"how do I get solar panels for free\"\n",
    "\n",
    "qry_tensor, qry_len = triplet_dataset.embed_text(query, triplet_dataset.query_max_len)\n",
    "qry_tensor = qry_tensor.unsqueeze(0).to(device)\n",
    "qry_len = torch.tensor([qry_len], device=device)\n",
    "\n",
    "qry_vec = qryTower(qry_tensor, qry_len)  # [1, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db107c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(qry_vec, doc_matrix)  # [num_docs]\n",
    "\n",
    "# Get top 5 document indices\n",
    "top_k = 5\n",
    "top_indices = similarities.topk(top_k).indices.cpu().numpy()\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n🔍 Query: {query}\")\n",
    "print(f\"\\n📚 Top {top_k} most relevant passages:\")\n",
    "for i in top_indices:\n",
    "    print(f\"\\n🔸 Similarity: {similarities[i].item():.4f}\")\n",
    "    print(all_doc_texts[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
